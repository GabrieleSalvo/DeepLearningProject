{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from base import BaseDataLoader\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import random\n",
    "from gensim.parsing.preprocessing import remove_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch import NeuralNetClassifier\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmznDataLoader():\n",
    "    def __init__(self):\n",
    "        df = self.getDF('./data/reviews_Amazon_Instant_Video_5.json.gz')\n",
    "#         display(df)\n",
    "#         print(self.df.loc[0][['overall','reviewText']])\n",
    "        df = df[['reviewText', 'overall']]\n",
    "        df['reviewText'] = df['reviewText'].apply(lambda x : self.title_parsing(x))\n",
    "        X = df['reviewText']\n",
    "        y = df['overall']-1\n",
    "        self.weight_matrix = self.get_weight_matrix(X)\n",
    "        _, self.idx2word = self.buildCorpus(X)\n",
    "        X = self.indicesMatrix(X)\n",
    "        self.data_features = X\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    def get_attention_mask(self):\n",
    "        index_matrix = data_loader.data_features\n",
    "        mask = (index_matrix!=0)\n",
    "        mask[:,-1] = False\n",
    "        attention_weigths = np.random.random(index_matrix.shape).round(1) * 2 + 3\n",
    "        attention_weigths[~mask] = float('-inf')\n",
    "        attention_weigths = softmax(attention_weigths, axis=1)\n",
    "        return attention_weigths\n",
    "\n",
    "    def title_parsing(self, title):  \n",
    "        # remove stop words and tokenization \n",
    "        title = re.sub('[^a-zA-Z]', ' ', str(title))\n",
    "        title = title.lower()\n",
    "        title = remove_stopwords(title)  ## remove stop words, corpus size 52680            \n",
    "        title = title.split()\n",
    "        title = [word for word in title if len(word) >1 ]\n",
    "        return title\n",
    "\n",
    "    def parse(self, path):\n",
    "        g = gzip.open(path, 'rb')\n",
    "        for l in g:\n",
    "            yield eval(l)\n",
    "\n",
    "    def getDF(self, path):\n",
    "        i = 0\n",
    "        df = {}\n",
    "        for d in self.parse(path):\n",
    "            df[i] = d\n",
    "            i += 1\n",
    "        return pd.DataFrame.from_dict(df, orient='index')        \n",
    "\n",
    "    def buildCorpus(self, X):\n",
    "        '''\n",
    "        return a dictionary with 'word' and its index in corpus as key and value respectively\n",
    "        '''\n",
    "        word2idx = {}\n",
    "        idx2word = [] ## alternatively use if.. condition\n",
    "        idx = 0 \n",
    "        for row in X:\n",
    "            for word in row:\n",
    "                if word not in word2idx:\n",
    "                    idx2word.append(word)                \n",
    "                    word2idx[word] = len(idx2word) - 1\n",
    "#             pickle.dump(word2idx, open('./data/corpusDict.txt', 'wb'))\n",
    "        return word2idx, idx2word  \n",
    "\n",
    "\n",
    "    def indicesMatrix(self, X):\n",
    "        '''\n",
    "        return matrix (num_reviews, maxNumberWords) such that review text transformed to index\n",
    "        '''\n",
    "        word2idx, _ = self.buildCorpus(X)\n",
    "#             word2idx = pickle.load(open('./data/corpusDict.txt', 'rb'))\n",
    "        ## 53008 words in corpus\n",
    "\n",
    "        corpusSize = len(word2idx) \n",
    "\n",
    "        maxNumberWords = sorted(len(x) for x in X)[-1]\n",
    "#             print (\"maximum\", maxNumberWords)\n",
    "\n",
    "        index_matrix = np.zeros((X.shape[0], maxNumberWords))          \n",
    "        for i, row in enumerate(X):\n",
    "            for j, word in enumerate(row):\n",
    "#                 try:\n",
    "#                     index_matrix[i,j] = word2idx[word]\n",
    "#                     words_found += 1\n",
    "#                 except KeyError:\n",
    "#                     index_matrix[i,j] = corpusSize     \n",
    "                index_matrix[i,j] = word2idx[word]\n",
    "        if maxNumberWords % 2 == 1:\n",
    "            x0 = np.full((index_matrix.shape[0], 1), maxNumberWords)\n",
    "            index_matrix = np.hstack((index_matrix, x0))\n",
    "        return index_matrix\n",
    "\n",
    "    def get_weight_matrix(self, X):\n",
    "        '''\n",
    "        return matrix contains embedding for word in corpus/review text\n",
    "        Note that the word cannot be found in the glove returns ?? as embedding\n",
    "        '''\n",
    "\n",
    "        glove = {}\n",
    "\n",
    "        with open(f'./data/glove.6B.50d.txt', 'rb') as f:\n",
    "            for l in f:\n",
    "                line = l.decode().split()           \n",
    "                word = line[0]\n",
    "    #            words.append(word)\n",
    "    #             word2idx[word] = idx\n",
    "    #            idx += 1\n",
    "                vect = np.array(line[1:]).astype(np.float)\n",
    "    #             vectors.append(vect)\n",
    "    #     vectors = np.reshape(vectors, (400000, 50))\n",
    "                glove.update({word:vect})\n",
    "    #     glove = {word2idx[w]: vectors[word2idx[w]] for w in words}  # alternatively generate weights_matrix directly\n",
    "\n",
    "        target_vocab, _ = self.buildCorpus(X)\n",
    "        #except\n",
    "        #exceptKey = list(set(list(glove.keys())).difference(list(target_vocab.keys())))  ## \n",
    "        matrix_len = len(target_vocab)\n",
    "        weights_matrix = np.zeros((matrix_len, 50))\n",
    "        words_found = 0\n",
    "        words_not_found = 0\n",
    "        for i, word in enumerate(target_vocab):\n",
    "            try: \n",
    "                weights_matrix[i] = glove[word]\n",
    "                words_found += 1\n",
    "            except KeyError:\n",
    "                words_not_found += 1\n",
    "                weights_matrix[i] = np.random.normal(scale=0.6, size=(50,))\n",
    "\n",
    "        print(words_not_found)\n",
    "        return  weights_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9776\n"
     ]
    }
   ],
   "source": [
    "data_loader = AmznDataLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>helpful</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A11N155CW1UV02</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>AdrianaM</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>I had big expectations because I love English ...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>A little bit boring for me</td>\n",
       "      <td>1399075200</td>\n",
       "      <td>05 3, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A3BC8O2KCL29V2</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>Carol T</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>I highly recommend this series. It is a must f...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Excellent Grown Up TV</td>\n",
       "      <td>1346630400</td>\n",
       "      <td>09 3, 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A60D5HQFOTSOM</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>Daniel Cooper \"dancoopermedia\"</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>This one is a real snoozer. Don't believe anyt...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Way too boring for me</td>\n",
       "      <td>1381881600</td>\n",
       "      <td>10 16, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A1RJPIGRSNX4PW</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>J. Kaplan \"JJ\"</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>Mysteries are interesting.  The tension betwee...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Robson Green is mesmerizing</td>\n",
       "      <td>1383091200</td>\n",
       "      <td>10 30, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A16XRPF40679KG</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>Michael Dobey</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>This show always is excellent, as far as briti...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Robson green and great writing</td>\n",
       "      <td>1234310400</td>\n",
       "      <td>02 11, 2009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID        asin                    reviewerName helpful  \\\n",
       "0  A11N155CW1UV02  B000H00VBQ                        AdrianaM  [0, 0]   \n",
       "1  A3BC8O2KCL29V2  B000H00VBQ                         Carol T  [0, 0]   \n",
       "2   A60D5HQFOTSOM  B000H00VBQ  Daniel Cooper \"dancoopermedia\"  [0, 1]   \n",
       "3  A1RJPIGRSNX4PW  B000H00VBQ                  J. Kaplan \"JJ\"  [0, 0]   \n",
       "4  A16XRPF40679KG  B000H00VBQ                   Michael Dobey  [1, 1]   \n",
       "\n",
       "                                          reviewText  overall  \\\n",
       "0  I had big expectations because I love English ...      2.0   \n",
       "1  I highly recommend this series. It is a must f...      5.0   \n",
       "2  This one is a real snoozer. Don't believe anyt...      1.0   \n",
       "3  Mysteries are interesting.  The tension betwee...      4.0   \n",
       "4  This show always is excellent, as far as briti...      5.0   \n",
       "\n",
       "                          summary  unixReviewTime   reviewTime  \n",
       "0      A little bit boring for me      1399075200   05 3, 2014  \n",
       "1           Excellent Grown Up TV      1346630400   09 3, 2012  \n",
       "2           Way too boring for me      1381881600  10 16, 2013  \n",
       "3     Robson Green is mesmerizing      1383091200  10 30, 2013  \n",
       "4  Robson green and great writing      1234310400  02 11, 2009  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = data_loader.getDF('./data/reviews_Amazon_Instant_Video_5.json.gz')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "data_loader.idx2word\n",
    "corpus = os.linesep.join(data_loader.idx2word)\n",
    "print(type(corpus))\n",
    "with open(\"text_for_bert.txt\", \"w\", encoding='utf-8') as fp:\n",
    "    fp.write(corpus)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02755311660771048"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(data_loader.X_train)/(data_loader.X_train.shape[0]*data_loader.X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29700, 1502)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader.X_train.shape\n",
    "# data_loader.buildCorpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.086395263671875\n"
     ]
    }
   ],
   "source": [
    "weight_matrix = data_loader.weight_matrix\n",
    "import sys\n",
    "print(sys.getsizeof(weight_matrix)/(2**20))\n",
    "\n",
    "# print(weight_matrix[0])\n",
    "# print(weight_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(weight_matrix,  open('./data/GloveMatrix.npy', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo reset parameters for linear layer\n",
    "class ReviewModel(nn.Module):\n",
    "    def __init__(self, max_length, batch_size):\n",
    "        super(ReviewModel, self).__init__()\n",
    "        weights_matrix = data_loader.weight_matrix\n",
    "        self.max_length = max_length\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.tensor(weights_matrix), freeze=False)\n",
    "#        self.embedding = nn.Embedding(weights_matrix.shape[0],weights_matrix.shape[1])\n",
    "#         self.embedding.weight.data.copy_(torch.from_numpy(weights_matrix))\n",
    "        self.drop1 = nn.Dropout(p=0.5)\n",
    "        self.conv1 = nn.Conv1d(in_channels=50, out_channels=200, kernel_size=4, padding=2).double()\n",
    "        self.conv2 = nn.Conv1d(in_channels=50, out_channels=200, kernel_size=5, padding=2).double()\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=2)\n",
    "        self.drop2 = nn.Dropout(p=0.15)\n",
    "        \n",
    "        self.rnn = nn.GRU(input_size=400, hidden_size=100, num_layers=1, batch_first=False)  #\n",
    "#        self.fc1 =nn.Linear(in_features=1501*100, out_features = 400)\n",
    "        self.fc1 =nn.Linear(in_features=(max_length//2)*100, out_features = 400)\n",
    "        self.drop3 = nn.Dropout(p=0.1)\n",
    "        self.fc2 = nn.Linear(in_features=400, out_features=5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "#         print(\"input tensor\", x.size())\n",
    "        x = self.embedding(x)\n",
    "#         print(\"after embedding\", x.size())\n",
    "        x = self.drop1(x)\n",
    "#        print(x)\n",
    "        x = x.view(-1,50,self.max_length)  # input(N,C_in,L) to conv1d\n",
    "        x1 = F.relu(self.conv1(x))\n",
    "#         print(\"x1 shape\", x1.size())\n",
    "        x2 = F.relu(self.conv2(x))\n",
    "#         print(\"x2 shape \", x2.size())\n",
    "\n",
    "        x1 = self.maxpool(x1)\n",
    "#         print(\"x1 shape\", x1.size())        \n",
    "        x2 = self.maxpool(x2)\n",
    "#         print(\"x2 shape\", x2.size())        \n",
    "        x = torch.cat((x1,x2), 1)\n",
    "        x = self.drop2(x)\n",
    "        \n",
    "        x = x.view((self.max_length//2),-1, 400).float()#### change dimensionality\n",
    "        \n",
    "        hidden = Variable(torch.cuda.FloatTensor(1, batch_size, 100).uniform_()) \n",
    "        \n",
    "#        print(\"before rnn \", x.size(), \"hidden size \",hidden.size())\n",
    "\n",
    "        output, _ = self.rnn(x,hidden)\n",
    "#         print(\"AFTER rnn \",output.size())\n",
    "        \n",
    "        x = output.contiguous().view(-1, (self.max_length//2)*100)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.drop3(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "#     def create_emb_layer(self, weights_matrix, non_trainable=True):\n",
    "#         num_embeddings, embedding_dim = weights_matrix.shape\n",
    "#         emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
    "#         emb_layer.load_state_dict({'weight': weights_matrix})\n",
    "#         if non_trainable:\n",
    "#             emb_layer.weight.requires_grad = False\n",
    "\n",
    "#         return emb_layer, num_embeddings, embedding_dim\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pytorch_pretrained_bert import BertTokenizer, BertModel\n",
    "# from flair.embeddings import BertEmbeddings\n",
    "# bert_embedding = BertEmbeddings('bert-base-uncased')\n",
    "# from flair.data import Sentence\n",
    "# sentence = Sentence('hello world')\n",
    "# emb = bert_embedding.embed(sentence)\n",
    "# for token in emb:\n",
    "#     print(token.embedding.detach().cpu().numpy().shape)\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# model = BertModel.from_pretrained('bert-base-uncased')\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewModelwithBert(nn.Module):\n",
    "    def __init__(self, max_length, batch_size, weights_matrix):\n",
    "        super(ReviewModelwithBert, self).__init__()\n",
    "#         weights_matrix = data_loader.weight_matrix\n",
    "        self.max_length = max_length\n",
    "        self.batch_size = batch_size\n",
    "#         self.embedding = nn.Embedding.from_pretrained(torch.tensor(weights_matrix), freeze=False)\n",
    "        vocab_size =  weights_matrix.shape[0]\n",
    "        embedding_dim = weights_matrix.shape[1]\n",
    "        self.embedding = nn.Embedding(weights_matrix.shape[0],weights_matrix.shape[1]).double()\n",
    "#         self.embedding.weight.data.copy_(torch.from_numpy(weights_matrix))\n",
    "        self.drop1 = nn.Dropout(p=0.5)\n",
    "        self.conv1 = nn.Conv1d(in_channels=50, out_channels=200, kernel_size=4, padding=2).double()\n",
    "        self.conv2 = nn.Conv1d(in_channels=50, out_channels=200, kernel_size=5, padding=2).double()\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=2)\n",
    "        self.drop2 = nn.Dropout(p=0.15)\n",
    "        \n",
    "        self.rnn = nn.GRU(input_size=400, hidden_size=100, num_layers=1, batch_first=False)  #\n",
    "#        self.fc1 =nn.Linear(in_features=1501*100, out_features = 400)\n",
    "        self.fc1 =nn.Linear(in_features=(max_length//2)*100, out_features = 400)\n",
    "        self.drop3 = nn.Dropout(p=0.1)\n",
    "        self.fc2 = nn.Linear(in_features=400, out_features=5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "#         print(\"input tensor\", x.size())\n",
    "#         attn_weigths = F.softmax(x, dim=1)\n",
    "        x = self.embedding(x)\n",
    "#         print(\"after embedding\", x.size())\n",
    "        \n",
    "        x = self.drop1(x)\n",
    "#        print(x)\n",
    "        x = x.view(-1,50,self.max_length)  # input(N,C_in,L) to conv1d\n",
    "        x1 = F.relu(self.conv1(x))\n",
    "#         print(\"x1 shape\", x1.size())\n",
    "        x2 = F.relu(self.conv2(x))\n",
    "#         print(\"x2 shape \", x2.size())\n",
    "\n",
    "        x1 = self.maxpool(x1)\n",
    "#         print(\"x1 shape\", x1.size())        \n",
    "        x2 = self.maxpool(x2)\n",
    "#         print(\"x2 shape\", x2.size())        \n",
    "        x = torch.cat((x1,x2), 1)\n",
    "        x = self.drop2(x)\n",
    "        \n",
    "        x = x.view((self.max_length//2),-1, 400).float()#### change dimensionality\n",
    "        \n",
    "        hidden = Variable(torch.cuda.FloatTensor(1, batch_size, 100).uniform_()) \n",
    "        \n",
    "#        print(\"before rnn \", x.size(), \"hidden size \",hidden.size())\n",
    "\n",
    "        output, _ = self.rnn(x,hidden)\n",
    "#         print(\"AFTER rnn \",output.size())\n",
    "        \n",
    "        x = output.contiguous().view(-1, (self.max_length//2)*100)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.drop3(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def get_tensors(self, word):\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        # Tokenized input\n",
    "        tokenized_text = tokenizer.tokenize(word)\n",
    "\n",
    "        # Mask a token that we will try to predict back with `BertForMaskedLM`\n",
    "\n",
    "        # Convert token to vocabulary indices\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "        # Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\n",
    "        # print(len(tokenized_text))\n",
    "        tokens = []\n",
    "        input_type_ids = []\n",
    "        tokens.append(\"[CLS]\")\n",
    "        input_type_ids.append(0)\n",
    "        for token in tokenized_text:\n",
    "            tokens.append(token)\n",
    "            input_type_ids.append(0)\n",
    "        tokens.append(\"[SEP]\")\n",
    "        input_type_ids.append(0)\n",
    "\n",
    "        input_mask = [1] * len(input_ids)\n",
    "        seq_length  = 512\n",
    "        # print('input_ids ', len(input_ids))\n",
    "        # print('input_mask ', len(input_mask))\n",
    "        # print('input_type_ids ', len(input_type_ids))\n",
    "        # Zero-pad up to the sequence length.\n",
    "        while len(input_ids) < seq_length:\n",
    "            input_ids.append(0)\n",
    "        while len(input_mask) < seq_length:\n",
    "            input_mask.append(0)\n",
    "        while len(input_type_ids)  < seq_length:\n",
    "            input_type_ids.append(0)\n",
    "        assert len(input_ids) == seq_length\n",
    "        assert len(input_mask) == seq_length\n",
    "        assert len(input_type_ids) == seq_length, len(input_type_ids)\n",
    "        # Convert inputs to PyTorch tensors\n",
    "        input_ids = torch.tensor([input_ids])\n",
    "        input_mask = torch.tensor([input_mask])\n",
    "        input_type_ids = torch.tensor([input_type_ids])\n",
    "        return input_ids, input_mask, input_type_ids\n",
    "    \n",
    "    \n",
    "    def get_bert_embedding(self, word, model):\n",
    "        input_ids, input_mask, input_type_ids = self.get_tensors(word)\n",
    "\n",
    "#         print(type(indexed_tokens))\n",
    "#         for _ in range(510-len(indexed_tokens)):\n",
    "#             indexed_tokens.append(0)\n",
    "#         segments_tensors = np.zeros((510))\n",
    "#         non_zeros = np.count_nonzero(indexed_tokens)\n",
    "#         segments_tensors[:non_zeros] = 1\n",
    "#         indexed_tokens = torch.tensor(indexed_tokens).long()\n",
    "#         segments_tensors = torch.tensor(segments_tensors).long()\n",
    "\n",
    "#         print(indexed_tokens.size())\n",
    "#         print(segments_tensors.size())\n",
    "        encoded_layers, pooled_output = model(input_ids, input_type_ids, input_mask)\n",
    "#         print(encoded_layers, pooled_output)\n",
    "        return pooled_output\n",
    "    def embedding_bert(self, x, seq_length, embedding_size, model):\n",
    "        #batch_size, seq-length\n",
    "        #transofrm into batch_size, seq_length, embedding size\n",
    "        #idx2word search\n",
    "        emb = torch.zeros(batch_size, seq_length, embedding_size)\n",
    "        for i in range(batch_size):\n",
    "            phrase_length = np.count_nonzero( x[i,:].detach().cpu().numpy())-1\n",
    "            for j in range(phrase_length):\n",
    "                word = data_loader.idx2word[x[i,j].detach().cpu().numpy().astype(np.int)]\n",
    "                emb[i, j,:] = self.get_bert_embedding(word, model)\n",
    "        return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch.callbacks import EpochScoring, PrintLog, ProgressBar, LRScheduler, EarlyStopping\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ds_accuracy(net, ds, y=None):\n",
    "    # assume ds yields (X, y), e.g. torchvision.datasets.MNIST\n",
    "    y_true = [y for _, y in ds]\n",
    "    y_pred = net.predict(ds)\n",
    "    return sklearn.metrics.accuracy_score(y_true, y_pred)\n",
    "\n",
    "\n",
    "callbacks = [\n",
    "    ('lrs',LRScheduler()),\n",
    "    ('est',EarlyStopping()) \n",
    "]\n",
    "\n",
    "batch_size = 256\n",
    "device = torch.device(\"cuda:0\")\n",
    "model = ReviewModelwithBert(data_loader.X_train.shape[1], batch_size, data_loader.weight_matrix).to(device)\n",
    "net = NeuralNetClassifier(model,## change dimensionality\n",
    "                          iterator_train__drop_last = True,\n",
    "                          iterator_valid__drop_last = True,\n",
    "                          iterator_train__shuffle = True,\n",
    "                          iterator_valid__shuffle = True,\n",
    "                          max_epochs=20, \n",
    "                          lr=0.01,\n",
    "                          criterion = nn.CrossEntropyLoss, \n",
    "                          optimizer=optim.Adam,\n",
    "                          batch_size = batch_size,\n",
    "                          callbacks = callbacks,\n",
    "                          device = torch.device('cuda:0')\n",
    "                          )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss       dur\n",
      "-------  ------------  -----------  ------------  --------\n",
      "      1      \u001b[36m237.4966\u001b[0m       \u001b[32m0.5617\u001b[0m        \u001b[35m1.2024\u001b[0m  195.9338\n",
      "      2        \u001b[36m1.2023\u001b[0m       0.5613        \u001b[35m1.2019\u001b[0m  195.7959\n",
      "      3        1.2029       0.5615        1.2032  196.6091\n",
      "      4        \u001b[36m1.2022\u001b[0m       \u001b[32m0.5620\u001b[0m        1.2037  195.8881\n",
      "      5        \u001b[36m1.2019\u001b[0m       \u001b[32m0.5627\u001b[0m        1.2019  195.7805\n",
      "      6        1.2025       0.5608        1.2043  195.9226\n",
      "      7        1.2019       0.5623        \u001b[35m1.2017\u001b[0m  196.7184\n",
      "      8        1.2020       \u001b[32m0.5628\u001b[0m        \u001b[35m1.2006\u001b[0m  197.0827\n",
      "      9        1.2020       0.5617        1.2025  197.1906\n",
      "     10        1.2022       0.5615        1.2023  195.0380\n",
      "     11        \u001b[36m1.2015\u001b[0m       0.5611        1.2038  197.0550\n",
      "     12        1.2031       0.5623        1.2026  196.1263\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<class 'skorch.classifier.NeuralNetClassifier'>[initialized](\n",
       "  module_=ReviewModelwithBert(\n",
       "    (embedding): Embedding(52655, 50)\n",
       "    (drop1): Dropout(p=0.5)\n",
       "    (conv1): Conv1d(50, 200, kernel_size=(4,), stride=(1,), padding=(2,))\n",
       "    (conv2): Conv1d(50, 200, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "    (maxpool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (drop2): Dropout(p=0.15)\n",
       "    (rnn): GRU(400, 100)\n",
       "    (fc1): Linear(in_features=75100, out_features=400, bias=True)\n",
       "    (drop3): Dropout(p=0.1)\n",
       "    (fc2): Linear(in_features=400, out_features=5, bias=True)\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = torch.from_numpy(data_loader.X_train).long().to(device)\n",
    "labels = torch.tensor(data_loader.y_train.values).long().to(device)\n",
    "\n",
    "# rest = inputs.size()[0] % batch_size\n",
    "# print(rest)\n",
    "# print(inputs.size())\n",
    "# inputs = inputs[:-rest]\n",
    "# print(inputs.size()[0]/batch_size)\n",
    "# print(inputs.shape)\n",
    "# labels = labels[:-rest]\n",
    "\n",
    "net.fit(inputs, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.from_numpy(data_loader.X_test).long().to(device)\n",
    "predictions = net.predict(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 122] Disk quota exceeded",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-9fda68b1b0dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m net.save_params(f_params = path+'model.pkl',\n\u001b[1;32m      3\u001b[0m                 \u001b[0mf_optimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'optimizer.pkl'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                 f_history = path+'history.json')\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/thesis/lib/python3.7/site-packages/skorch/net.py\u001b[0m in \u001b[0;36msave_params\u001b[0;34m(self, f, f_params, f_optimizer, f_history)\u001b[0m\n\u001b[1;32m   1480\u001b[0m                     \u001b[0;34m\"Please initialize first by calling .initialize() \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1481\u001b[0m                     \"or by fitting the model with .fit(...).\")\n\u001b[0;32m-> 1482\u001b[0;31m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1484\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf_optimizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/thesis/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \"\"\"\n\u001b[0;32m--> 224\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_with_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/thesis/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_with_file_like\u001b[0;34m(f, mode, body)\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_fd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 122] Disk quota exceeded"
     ]
    }
   ],
   "source": [
    "# path = './data/'\n",
    "# net.save_params(f_params = path+'model.pkl',\n",
    "#                 f_optimizer = path+'optimizer.pkl',\n",
    "#                 f_history = path+'history.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5660021551724138"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(data_loader.y_test[:-2], predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = data_loader.indicesMatrix()\n",
    "import sklearn\n",
    "sorted(sklearn.metrics.SCORERS.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_losses(tr_loss, val_loss):\n",
    "    plt.plot(tr_loss, label=\"training\")\n",
    "    plt.plot(val_loss, label=\"validation\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#todo plot losses\n",
    "#todo plot accuracy\n",
    "#todo confusion matrix\n",
    "history = net.history\n",
    "train_losses = history[:, 'train_loss']\n",
    "valid_losses = history[:, 'valid_loss']\n",
    "\n",
    "# accuracy = history[:, 'accuracy']\n",
    "plot_losses(train_losses, valid_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.tensor(weights_matrix).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Variable(w.double())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
