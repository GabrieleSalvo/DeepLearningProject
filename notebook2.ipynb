{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from base import BaseDataLoader\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import visdom\n",
    "import random\n",
    "from gensim.parsing.preprocessing import remove_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch import NeuralNetClassifier\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmznDataLoader():\n",
    "        def __init__(self):\n",
    "            df = self.getDF('./data/reviews_Amazon_Instant_Video_5.json.gz')\n",
    "    #         display(self.df)\n",
    "    #         print(self.df.loc[0][['overall','reviewText']])\n",
    "            df = df[['reviewText', 'overall']]\n",
    "            df['reviewText'] = df['reviewText'].apply(lambda x : self.title_parsing(x))\n",
    "            X = df['reviewText']\n",
    "            y = df['overall']\n",
    "            self.weight_matrix = self.get_weigth_matrix(X)\n",
    "            X = self.indicesMatrix(X)\n",
    "            self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "            \n",
    "        def title_parsing(self, title):  \n",
    "            # remove stop words and tokenization \n",
    "            title = re.sub('[^a-zA-Z]', ' ', str(title))\n",
    "            title = title.lower()\n",
    "            title = remove_stopwords(title)  ## remove stop words, corpus size 52680            \n",
    "            title = title.split()\n",
    "            title = [word for word in title if len(word) >1 ]\n",
    "            return title\n",
    "        \n",
    "        \n",
    "        def parse(self, path):\n",
    "            g = gzip.open(path, 'rb')\n",
    "            for l in g:\n",
    "                yield eval(l)\n",
    "            \n",
    "        def getDF(self, path):\n",
    "            i = 0\n",
    "            df = {}\n",
    "            for d in self.parse(path):\n",
    "                df[i] = d\n",
    "                i += 1\n",
    "            return pd.DataFrame.from_dict(df, orient='index')        \n",
    "\n",
    "        def buildCorpus(self, X):\n",
    "            '''\n",
    "            return a dictionary with 'word' and its index in corpus as key and value respectively\n",
    "            '''\n",
    "            word2idx = {}\n",
    "            idx2word = [] ## alternatively use if.. condition\n",
    "            idx = 0 \n",
    "            for row in X:\n",
    "                for word in row:\n",
    "                    if word not in word2idx:\n",
    "                        idx2word.append(word)                \n",
    "                        word2idx[word] = len(idx2word) - 1\n",
    "#             pickle.dump(word2idx, open('./data/corpusDict.txt', 'wb'))\n",
    "            return word2idx   \n",
    "        \n",
    "    \n",
    "        def indicesMatrix(self, X):\n",
    "            '''\n",
    "            return matrix (num_reviews, maxNumberWords) such that review text transformed to index\n",
    "            '''\n",
    "            word2idx = self.buildCorpus(X)\n",
    "#             word2idx = pickle.load(open('./data/corpusDict.txt', 'rb'))\n",
    "            ## 53008 words in corpus\n",
    "    \n",
    "            corpusSize = len(word2idx) \n",
    "        \n",
    "            maxNumberWords = sorted(len(x) for x in X)[-1]\n",
    "#             print (\"maximum\", maxNumberWords)\n",
    "\n",
    "            index_matrix = np.zeros((X.shape[0], maxNumberWords))          \n",
    "            for i, row in enumerate(X):\n",
    "                for j, word in enumerate(row):\n",
    "#                     try:\n",
    "#                         index_matrix[i,j] = word2idx[word]\n",
    "#                         words_found += 1\n",
    "#                     except KeyError:\n",
    "#                         index_matrix[i,j] = corpusSize     \n",
    "\n",
    "                    index_matrix[i,j] = word2idx[word]\n",
    "            if maxNumberWords % 2 == 1:\n",
    "                x0 = np.full((index_matrix.shape[0], 1), maxNumberWords)\n",
    "                index_matrix = np.hstack((index_matrix, x0))\n",
    "            return index_matrix\n",
    "        \n",
    "        def get_weigth_matrix(self, X):\n",
    "            '''\n",
    "            return matrix contains embedding for word in corpus/review text\n",
    "            Note that the word cannot be found in the glove returns ?? as embedding\n",
    "            '''\n",
    "\n",
    "            glove = {}\n",
    "\n",
    "            with open(f'./data/glove.6B.50d.txt', 'rb') as f:\n",
    "                for l in f:\n",
    "                    line = l.decode().split()           \n",
    "                    word = line[0]\n",
    "        #            words.append(word)\n",
    "        #             word2idx[word] = idx\n",
    "        #            idx += 1\n",
    "                    vect = np.array(line[1:]).astype(np.float)\n",
    "        #             vectors.append(vect)\n",
    "        #     vectors = np.reshape(vectors, (400000, 50))\n",
    "                    glove.update({word:vect})\n",
    "        #     glove = {word2idx[w]: vectors[word2idx[w]] for w in words}  # alternatively generate weights_matrix directly\n",
    "\n",
    "            target_vocab = self.buildCorpus(X)\n",
    "            #except\n",
    "            #exceptKey = list(set(list(glove.keys())).difference(list(target_vocab.keys())))  ## \n",
    "            matrix_len = len(target_vocab)\n",
    "            weights_matrix = np.zeros((matrix_len, 50))\n",
    "            words_found = 0\n",
    "            words_not_found = 0\n",
    "            for i, word in enumerate(target_vocab):\n",
    "                try: \n",
    "                    weights_matrix[i] = glove[word]\n",
    "                    words_found += 1\n",
    "                except KeyError:\n",
    "                    words_not_found += 1\n",
    "                    weights_matrix[i] = np.random.normal(scale=0.6, size=(50,))\n",
    "\n",
    "            print(words_not_found)\n",
    "            return  weights_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9776\n"
     ]
    }
   ],
   "source": [
    "data_loader = AmznDataLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29700, 1502)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader.X_train.shape\n",
    "# data_loader.buildCorpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.086395263671875\n"
     ]
    }
   ],
   "source": [
    "weight_matrix = data_loader.weight_matrix\n",
    "import sys\n",
    "print(sys.getsizeof(weight_matrix)/(2**20))\n",
    "\n",
    "# print(weight_matrix[0])\n",
    "# print(weight_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(weight_matrix,  open('./data/GloveMatrix.npy', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo reset parameters for linear layer\n",
    "class ReviewModel(nn.Module):\n",
    "    def __init__(self, max_length, batch_size):\n",
    "        super(ReviewModel, self).__init__()\n",
    "        weights_matrix = data_loader.weight_matrix\n",
    "        self.max_length = max_length\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.tensor(weights_matrix), freeze=False)\n",
    "#        self.embedding = nn.Embedding(weights_matrix.shape[0],weights_matrix.shape[1])\n",
    "#         self.embedding.weight.data.copy_(torch.from_numpy(weights_matrix))\n",
    "        self.drop1 = nn.Dropout(p=0.5)\n",
    "        self.conv1 = nn.Conv1d(in_channels=50, out_channels=200, kernel_size=4, padding=2).double()\n",
    "        self.conv2 = nn.Conv1d(in_channels=50, out_channels=200, kernel_size=5, padding=2).double()\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=2)\n",
    "        self.drop2 = nn.Dropout(p=0.15)\n",
    "        \n",
    "        self.rnn = nn.GRU(input_size=400, hidden_size=100, num_layers=1, batch_first=False)  #\n",
    "#        self.fc1 =nn.Linear(in_features=1501*100, out_features = 400)\n",
    "        self.fc1 =nn.Linear(in_features=(max_length//2)*100, out_features = 400)\n",
    "        self.drop3 = nn.Dropout(p=0.1)\n",
    "        self.fc2 = nn.Linear(in_features=400, out_features=10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "#         print(\"input tensor\", x.size())\n",
    "        x = self.embedding(x)\n",
    "#         print(\"after embedding\", x.size())\n",
    "        x = self.drop1(x)\n",
    "#        print(x)\n",
    "        x = x.view(-1,50,self.max_length)  # input(N,C_in,L) to conv1d\n",
    "        x1 = F.relu(self.conv1(x))\n",
    "#         print(\"x1 shape\", x1.size())\n",
    "        x2 = F.relu(self.conv2(x))\n",
    "#         print(\"x2 shape \", x2.size())\n",
    "\n",
    "        x1 = self.maxpool(x1)\n",
    "#         print(\"x1 shape\", x1.size())        \n",
    "        x2 = self.maxpool(x2)\n",
    "#         print(\"x2 shape\", x2.size())        \n",
    "        x = torch.cat((x1,x2), 1)\n",
    "        x = self.drop2(x)\n",
    "        \n",
    "        x = x.view((self.max_length//2),-1, 400).float()#### change dimensionality\n",
    "        \n",
    "        hidden = Variable(torch.cuda.FloatTensor(1, batch_size, 100).uniform_()) \n",
    "        \n",
    "#        print(\"before rnn \", x.size(), \"hidden size \",hidden.size())\n",
    "\n",
    "        output, _ = self.rnn(x,hidden)\n",
    "#         print(\"AFTER rnn \",output.size())\n",
    "        \n",
    "        x = output.contiguous().view(-1, (self.max_length//2)*100)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.drop3(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "#     def create_emb_layer(self, weights_matrix, non_trainable=True):\n",
    "#         num_embeddings, embedding_dim = weights_matrix.shape\n",
    "#         emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
    "#         emb_layer.load_state_dict({'weight': weights_matrix})\n",
    "#         if non_trainable:\n",
    "#             emb_layer.weight.requires_grad = False\n",
    "\n",
    "#         return emb_layer, num_embeddings, embedding_dim\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch.callbacks import EpochScoring, PrintLog, ProgressBar, LRScheduler, EarlyStopping\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ds_accuracy(net, ds, y=None):\n",
    "    # assume ds yields (X, y), e.g. torchvision.datasets.MNIST\n",
    "    y_true = [y for _, y in ds]\n",
    "    y_pred = net.predict(ds)\n",
    "    return sklearn.metrics.accuracy_score(y_true, y_pred)\n",
    "    \n",
    "\n",
    "callbacks = [\n",
    "    ('es1',EpochScoring('accuracy')),\n",
    "    ('lrs',LRScheduler()),\n",
    "    ('est',EarlyStopping()) \n",
    "]\n",
    "\n",
    "batch_size = 256\n",
    "device = torch.device(\"cuda:0\")\n",
    "model = ReviewModel(data_loader.X_train.shape[1], batch_size).to(device)\n",
    "net = NeuralNetClassifier(model,## change dimensionality\n",
    "                          iterator_train__drop_last = True,\n",
    "                          iterator_valid__drop_last = True, \n",
    "                          max_epochs=10, \n",
    "                          lr=0.001, \n",
    "                          criterion = nn.CrossEntropyLoss, \n",
    "                          optimizer=optim.SGD,\n",
    "                          optimizer__param_groups=[('momentum', 0.9)],\n",
    "                          batch_size = batch_size,\n",
    "                          callbacks = callbacks,\n",
    "                          device = torch.device('cuda:0')\n",
    "                          )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f4329b86fd14a03bd4f94530fde151f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=117), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    accuracy    train_loss    valid_acc    valid_loss       dur\n",
      "-------  ----------  ------------  -----------  ------------  --------\n",
      "      1      \u001b[36m0.5669\u001b[0m      \u001b[32m104.1349\u001b[0m       \u001b[35m0.5669\u001b[0m        \u001b[31m1.5946\u001b[0m  192.1774\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "594c09fb300847e29bd1f01486aeb9cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=117), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2      0.5669        \u001b[32m1.4923\u001b[0m       0.5669        \u001b[31m1.4140\u001b[0m  202.3074\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "794b5c03194d41969e73579801d59787",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=117), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      3      0.5669        \u001b[32m1.3872\u001b[0m       0.5669        \u001b[31m1.3538\u001b[0m  193.7004\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ec71e645e4e4fa9b8fc881e3aac07d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=117), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      4      0.5669        \u001b[32m1.3444\u001b[0m       0.5669        \u001b[31m1.3246\u001b[0m  188.0681\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3ff6e67985e402ea3a96866cb1bd48f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=117), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      5      0.5669        \u001b[32m1.3220\u001b[0m       0.5669        \u001b[31m1.3082\u001b[0m  186.8971\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6cf5be3157f4b6fb9a2c13e826c7c72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=117), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      6      0.5669        \u001b[32m1.3090\u001b[0m       0.5669        \u001b[31m1.2985\u001b[0m  186.6740\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73c7d097488a42078aa3bac6718f76b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=117), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      7      0.5669        \u001b[32m1.3013\u001b[0m       0.5669        \u001b[31m1.2928\u001b[0m  187.5319\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e6ab9bb7d7d4cc882906fea65351849",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=117), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      8      0.5669        \u001b[32m1.2969\u001b[0m       0.5669        \u001b[31m1.2897\u001b[0m  188.1601\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5773c9c86a5d40c79155d62636f56ffb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=117), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      9      0.5669        \u001b[32m1.2947\u001b[0m       0.5669        \u001b[31m1.2884\u001b[0m  188.8265\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e886d49bc7904a7fb1a03ae35f07197c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=117), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inputs = torch.from_numpy(data_loader.X_train).long().to(device)\n",
    "labels = torch.tensor(data_loader.y_train.values).long().to(device)\n",
    "# rest = inputs.size()[0] % batch_size\n",
    "# print(rest)\n",
    "# print(inputs.size())\n",
    "# inputs = inputs[:-rest]\n",
    "# print(inputs.size()[0]/batch_size)\n",
    "# print(inputs.shape)\n",
    "# labels = labels[:-rest]\n",
    "\n",
    "net.fit(inputs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'module': ReviewModel(\n",
       "   (embedding): Embedding(52655, 50)\n",
       "   (drop1): Dropout(p=0.5)\n",
       "   (conv1): Conv1d(50, 200, kernel_size=(4,), stride=(1,), padding=(2,))\n",
       "   (conv2): Conv1d(50, 200, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "   (maxpool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "   (drop2): Dropout(p=0.15)\n",
       "   (rnn): GRU(400, 100)\n",
       "   (fc1): Linear(in_features=75100, out_features=400, bias=True)\n",
       "   (drop3): Dropout(p=0.1)\n",
       "   (fc2): Linear(in_features=400, out_features=10, bias=True)\n",
       " ),\n",
       " 'criterion': torch.nn.modules.loss.CrossEntropyLoss,\n",
       " 'optimizer': torch.optim.sgd.SGD,\n",
       " 'lr': 0.001,\n",
       " 'max_epochs': 1,\n",
       " 'batch_size': 256,\n",
       " 'iterator_train': torch.utils.data.dataloader.DataLoader,\n",
       " 'iterator_valid': torch.utils.data.dataloader.DataLoader,\n",
       " 'dataset': skorch.dataset.Dataset,\n",
       " 'train_split': <skorch.dataset.CVSplit object at 0x7f2bfcf6e4a8>,\n",
       " 'callbacks': [('es1',\n",
       "   <skorch.callbacks.scoring.EpochScoring at 0x7f2bec627e80>),\n",
       "  ('pl', <skorch.callbacks.logging.PrintLog at 0x7f2bec6275c0>),\n",
       "  ('pb', <skorch.callbacks.logging.ProgressBar at 0x7f2bec627278>),\n",
       "  ('lrs', <skorch.callbacks.lr_scheduler.LRScheduler at 0x7f2bf4f2d7b8>),\n",
       "  ('est', <skorch.callbacks.training.EarlyStopping at 0x7f2bf4f2d518>)],\n",
       " 'warm_start': False,\n",
       " 'verbose': 1,\n",
       " 'device': device(type='cuda', index=0),\n",
       " 'iterator_train__drop_last': True,\n",
       " 'iterator_valid__drop_last': True,\n",
       " 'optimizer__param_groups': [('momentum', 0.9)],\n",
       " 'history': [{'batches': [{'train_loss': 2.248842239379883,\n",
       "     'train_batch_size': 256},\n",
       "    {'train_loss': 15.481327056884766, 'train_batch_size': 256},\n",
       "    {'train_loss': 7.180950164794922, 'train_batch_size': 256},\n",
       "    {'train_loss': 564.207275390625, 'train_batch_size': 256},\n",
       "    {'train_loss': 247.7823028564453, 'train_batch_size': 256},\n",
       "    {'train_loss': 168.6234588623047, 'train_batch_size': 256},\n",
       "    {'train_loss': 756.9953002929688, 'train_batch_size': 256},\n",
       "    {'train_loss': 2.259401559829712, 'train_batch_size': 256},\n",
       "    {'train_loss': 2.229581832885742, 'train_batch_size': 256},\n",
       "    {'train_loss': 2.882789373397827, 'train_batch_size': 256},\n",
       "    {'train_loss': 2.2376468181610107, 'train_batch_size': 256},\n",
       "    {'train_loss': 2.213200330734253, 'train_batch_size': 256},\n",
       "    {'train_loss': 2.206852674484253, 'train_batch_size': 256},\n",
       "    {'train_loss': 2.193817138671875, 'train_batch_size': 256},\n",
       "    {'train_loss': 2.1791892051696777, 'train_batch_size': 256},\n",
       "    {'train_loss': 2.167264699935913, 'train_batch_size': 256},\n",
       "    {'train_loss': 2.1716928482055664, 'train_batch_size': 256},\n",
       "    {'train_loss': 2.158365249633789, 'train_batch_size': 256},\n",
       "    {'train_loss': 2.14559006690979, 'train_batch_size': 256},\n",
       "    {'train_loss': 2.110135555267334, 'train_batch_size': 256},\n",
       "    {'train_loss': 2.0979135036468506, 'train_batch_size': 256},\n",
       "    {'train_loss': 2.0907557010650635, 'train_batch_size': 256},\n",
       "    {'train_loss': 2.080357789993286, 'train_batch_size': 256},\n",
       "    {'train_loss': 2.0731263160705566, 'train_batch_size': 256},\n",
       "    {'train_loss': 2.0357720851898193, 'train_batch_size': 256},\n",
       "    {'train_loss': 2.0374748706817627, 'train_batch_size': 256},\n",
       "    {'train_loss': 2.0418145656585693, 'train_batch_size': 256},\n",
       "    {'train_loss': 2.000293731689453, 'train_batch_size': 256},\n",
       "    {'train_loss': 2.0281074047088623, 'train_batch_size': 256},\n",
       "    {'train_loss': 1.9954835176467896, 'train_batch_size': 256},\n",
       "    {'train_loss': 2.0103018283843994, 'train_batch_size': 256},\n",
       "    {'train_loss': 1.9597315788269043, 'train_batch_size': 256},\n",
       "    {'train_loss': 1.9601123332977295, 'train_batch_size': 256},\n",
       "    {'train_loss': 1.9589604139328003, 'train_batch_size': 256},\n",
       "    {'train_loss': 1.9492106437683105, 'train_batch_size': 256},\n",
       "    {'train_loss': 1.9109281301498413, 'train_batch_size': 256},\n",
       "    {'train_loss': 1.9410473108291626, 'train_batch_size': 256},\n",
       "    {'train_loss': 1.9190515279769897, 'train_batch_size': 256},\n",
       "    {'train_loss': 1.927498459815979, 'train_batch_size': 256},\n",
       "    {'train_loss': 1.8978745937347412, 'train_batch_size': 256},\n",
       "    {'train_loss': 1.8733028173446655, 'train_batch_size': 256},\n",
       "    {'train_loss': 1.893395185470581, 'train_batch_size': 256},\n",
       "    {'train_loss': 1.8803598880767822, 'train_batch_size': 256},\n",
       "    {'train_loss': 1.9145551919937134, 'train_batch_size': 256},\n",
       "    {'train_loss': 1.8467999696731567, 'train_batch_size': 256},\n",
       "    {'train_loss': 1.8817189931869507, 'train_batch_size': 256},\n",
       "    {'train_loss': 1.8400473594665527, 'train_batch_size': 256},\n",
       "    {'train_loss': 1.8601548671722412, 'train_batch_size': 256},\n",
       "    {'train_loss': 1.8070694208145142, 'train_batch_size': 256},\n",
       "    {'train_loss': 1.8288382291793823, 'train_batch_size': 256},\n",
       "    {'train_loss': 1.7895143032073975, 'train_batch_size': 256},\n",
       "    {'train_loss': 1.8171359300613403, 'train_batch_size': 256},\n",
       "    {'train_loss': 1.852689504623413, 'train_batch_size': 256},\n",
       "    {'train_loss': 1.7624510526657104, 'train_batch_size': 256},\n",
       "    {'train_loss': 1.7864147424697876, 'train_batch_size': 256},\n",
       "    {'train_loss': 1.8008503913879395, 'train_batch_size': 256},\n",
       "    {'train_loss': 1.8052393198013306, 'train_batch_size': 256},\n",
       "    {'train_loss': 1.7665901184082031, 'train_batch_size': 256},\n",
       "    {'train_loss': 1.798488974571228, 'train_batch_size': 256},\n",
       "    {'train_loss': 1.7053216695785522, 'train_batch_size': 256},\n",
       "    {'train_loss': 1.738307237625122, 'train_batch_size': 256},\n",
       "    {'train_loss': 1.7460122108459473, 'train_batch_size': 256},\n",
       "    {'train_loss': 1.7116384506225586, 'train_batch_size': 256},\n",
       "    {'train_loss': 1.7469048500061035, 'train_batch_size': 256},\n",
       "    {'train_loss': 1.6936614513397217, 'train_batch_size': 256},\n",
       "    {'train_loss': 1.7019914388656616, 'train_batch_size': 256},\n",
       "    {'train_loss': 1.7642325162887573, 'train_batch_size': 256},\n",
       "    {'train_loss': 1.679646372795105, 'train_batch_size': 256},\n",
       "    {'train_loss': 1.6658034324645996, 'train_batch_size': 256},\n",
       "    {'train_loss': 1.7128154039382935, 'train_batch_size': 256},\n",
       "    {'train_loss': 1.744706153869629, 'train_batch_size': 256},\n",
       "    {'train_loss': 1.6461790800094604, 'train_batch_size': 256},\n",
       "    {'train_loss': 1.626381754875183, 'train_batch_size': 256},\n",
       "    {'train_loss': 1.668414831161499, 'train_batch_size': 256},\n",
       "    {'train_loss': 1.6983014345169067, 'train_batch_size': 256},\n",
       "    {'train_loss': 1.6399376392364502, 'train_batch_size': 256},\n",
       "    {'train_loss': 1.628248929977417, 'train_batch_size': 256},\n",
       "    {'train_loss': 1.6353737115859985, 'train_batch_size': 256},\n",
       "    {'train_loss': 1.6320538520812988, 'train_batch_size': 256},\n",
       "    {'train_loss': 1.6319880485534668, 'train_batch_size': 256},\n",
       "    {'train_loss': 1.5907353162765503, 'train_batch_size': 256},\n",
       "    {'train_loss': 1.622589111328125, 'train_batch_size': 256},\n",
       "    {'train_loss': 1.535607099533081, 'train_batch_size': 256},\n",
       "    {'train_loss': 1.657180666923523, 'train_batch_size': 256},\n",
       "    {'train_loss': 1.5766047239303589, 'train_batch_size': 256},\n",
       "    {'train_loss': 1.6429095268249512, 'train_batch_size': 256},\n",
       "    {'train_loss': 1.6228471994400024, 'train_batch_size': 256},\n",
       "    {'train_loss': 1.704890489578247, 'train_batch_size': 256},\n",
       "    {'train_loss': 1.62098228931427, 'train_batch_size': 256},\n",
       "    {'train_loss': 1.5899635553359985, 'train_batch_size': 256},\n",
       "    {'train_loss': 1.5628770589828491, 'train_batch_size': 256},\n",
       "    {'train_loss': 1.544708251953125, 'train_batch_size': 256},\n",
       "    {'valid_loss': 1.5780417919158936, 'valid_batch_size': 256},\n",
       "    {'valid_loss': 1.6189814805984497, 'valid_batch_size': 256},\n",
       "    {'valid_loss': 1.56901216506958, 'valid_batch_size': 256},\n",
       "    {'valid_loss': 1.5625567436218262, 'valid_batch_size': 256},\n",
       "    {'valid_loss': 1.5529865026474, 'valid_batch_size': 256},\n",
       "    {'valid_loss': 1.575639009475708, 'valid_batch_size': 256},\n",
       "    {'valid_loss': 1.5694853067398071, 'valid_batch_size': 256},\n",
       "    {'valid_loss': 1.5591979026794434, 'valid_batch_size': 256},\n",
       "    {'valid_loss': 1.6019803285598755, 'valid_batch_size': 256},\n",
       "    {'valid_loss': 1.600147008895874, 'valid_batch_size': 256},\n",
       "    {'valid_loss': 1.5376578569412231, 'valid_batch_size': 256},\n",
       "    {'valid_loss': 1.651780366897583, 'valid_batch_size': 256},\n",
       "    {'valid_loss': 1.5774900913238525, 'valid_batch_size': 256},\n",
       "    {'valid_loss': 1.6569225788116455, 'valid_batch_size': 256},\n",
       "    {'valid_loss': 1.5691152811050415, 'valid_batch_size': 256},\n",
       "    {'valid_loss': 1.5480307340621948, 'valid_batch_size': 256},\n",
       "    {'valid_loss': 1.6147366762161255, 'valid_batch_size': 256},\n",
       "    {'valid_loss': 1.553497552871704, 'valid_batch_size': 256},\n",
       "    {'valid_loss': 1.6157094240188599, 'valid_batch_size': 256},\n",
       "    {'valid_loss': 1.5441136360168457, 'valid_batch_size': 256},\n",
       "    {'valid_loss': 1.6072674989700317, 'valid_batch_size': 256},\n",
       "    {'valid_loss': 1.5390478372573853, 'valid_batch_size': 256},\n",
       "    {'valid_loss': 1.5778236389160156, 'valid_batch_size': 256}],\n",
       "   'epoch': 1,\n",
       "   'dur': 186.9112548828125,\n",
       "   'train_loss': 20.881372092858605,\n",
       "   'train_loss_best': True,\n",
       "   'valid_loss': 1.5817922353744507,\n",
       "   'valid_loss_best': True,\n",
       "   'valid_acc': 0.5669157608695652,\n",
       "   'valid_acc_best': True,\n",
       "   'accuracy': 0.5669157608695652,\n",
       "   'accuracy_best': True}],\n",
       " 'initialized_': True,\n",
       " 'virtual_params_': {'optimizer__param_groups__*__*': <function skorch.setter.optimizer_setter(net, param, value, optimizer_attr='optimizer_', optimizer_name='optimizer')>,\n",
       "  'optimizer__*': <function skorch.setter.optimizer_setter(net, param, value, optimizer_attr='optimizer_', optimizer_name='optimizer')>,\n",
       "  'lr': <function skorch.setter.optimizer_setter(net, param, value, optimizer_attr='optimizer_', optimizer_name='optimizer')>},\n",
       " 'callbacks_': [('epoch_timer',\n",
       "   <skorch.callbacks.logging.EpochTimer at 0x7f2bf0a5b630>),\n",
       "  ('train_loss', <skorch.callbacks.scoring.BatchScoring at 0x7f2bf0a5b828>),\n",
       "  ('valid_loss', <skorch.callbacks.scoring.BatchScoring at 0x7f2bf0a5b7f0>),\n",
       "  ('valid_acc', <skorch.callbacks.scoring.EpochScoring at 0x7f2bf0a5b780>),\n",
       "  ('es1', <skorch.callbacks.scoring.EpochScoring at 0x7f2bec627e80>),\n",
       "  ('pb', <skorch.callbacks.logging.ProgressBar at 0x7f2bec627278>),\n",
       "  ('lrs', <skorch.callbacks.lr_scheduler.LRScheduler at 0x7f2bf4f2d7b8>),\n",
       "  ('est', <skorch.callbacks.training.EarlyStopping at 0x7f2bf4f2d518>),\n",
       "  ('print_log', <skorch.callbacks.logging.PrintLog at 0x7f2bf0a5b7b8>),\n",
       "  ('pl', <skorch.callbacks.logging.PrintLog at 0x7f2bec6275c0>)],\n",
       " 'criterion_': CrossEntropyLoss(),\n",
       " 'module_': ReviewModel(\n",
       "   (embedding): Embedding(52655, 50)\n",
       "   (drop1): Dropout(p=0.5)\n",
       "   (conv1): Conv1d(50, 200, kernel_size=(4,), stride=(1,), padding=(2,))\n",
       "   (conv2): Conv1d(50, 200, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "   (maxpool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "   (drop2): Dropout(p=0.15)\n",
       "   (rnn): GRU(400, 100)\n",
       "   (fc1): Linear(in_features=75100, out_features=400, bias=True)\n",
       "   (drop3): Dropout(p=0.1)\n",
       "   (fc2): Linear(in_features=400, out_features=10, bias=True)\n",
       " ),\n",
       " 'optimizer_': SGD (\n",
       " Parameter Group 0\n",
       "     dampening: 0\n",
       "     initial_lr: 0.001\n",
       "     lr: 0.05\n",
       "     momentum: 0\n",
       "     nesterov: False\n",
       "     weight_decay: 0\n",
       " ),\n",
       " 'callbacks__epoch_timer': <skorch.callbacks.logging.EpochTimer at 0x7f2c0d62fd68>,\n",
       " 'callbacks__train_loss': <skorch.callbacks.scoring.BatchScoring at 0x7f2c0d62fe48>,\n",
       " 'callbacks__train_loss__scoring': <function skorch.utils.train_loss_score(net, X=None, y=None)>,\n",
       " 'callbacks__train_loss__lower_is_better': True,\n",
       " 'callbacks__train_loss__on_train': True,\n",
       " 'callbacks__train_loss__name': 'train_loss',\n",
       " 'callbacks__train_loss__target_extractor': <function skorch.utils.noop(*args, **kwargs)>,\n",
       " 'callbacks__train_loss__use_caching': True,\n",
       " 'callbacks__valid_loss': <skorch.callbacks.scoring.BatchScoring at 0x7f2c0d62fb00>,\n",
       " 'callbacks__valid_loss__scoring': <function skorch.utils.valid_loss_score(net, X=None, y=None)>,\n",
       " 'callbacks__valid_loss__lower_is_better': True,\n",
       " 'callbacks__valid_loss__on_train': False,\n",
       " 'callbacks__valid_loss__name': 'valid_loss',\n",
       " 'callbacks__valid_loss__target_extractor': <function skorch.utils.noop(*args, **kwargs)>,\n",
       " 'callbacks__valid_loss__use_caching': True,\n",
       " 'callbacks__valid_acc': <skorch.callbacks.scoring.EpochScoring at 0x7f2c0d62f940>,\n",
       " 'callbacks__valid_acc__scoring': 'accuracy',\n",
       " 'callbacks__valid_acc__lower_is_better': False,\n",
       " 'callbacks__valid_acc__on_train': False,\n",
       " 'callbacks__valid_acc__name': 'valid_acc',\n",
       " 'callbacks__valid_acc__target_extractor': <function skorch.utils.to_numpy(X)>,\n",
       " 'callbacks__valid_acc__use_caching': True,\n",
       " 'callbacks__es1': <skorch.callbacks.scoring.EpochScoring at 0x7f2bec627e80>,\n",
       " 'callbacks__es1__scoring': 'accuracy',\n",
       " 'callbacks__es1__lower_is_better': True,\n",
       " 'callbacks__es1__on_train': False,\n",
       " 'callbacks__es1__name': None,\n",
       " 'callbacks__es1__target_extractor': <function skorch.utils.to_numpy(X)>,\n",
       " 'callbacks__es1__use_caching': True,\n",
       " 'callbacks__pb': <skorch.callbacks.logging.ProgressBar at 0x7f2bec627278>,\n",
       " 'callbacks__pb__batches_per_epoch': 'auto',\n",
       " 'callbacks__pb__detect_notebook': True,\n",
       " 'callbacks__pb__postfix_keys': ['train_loss', 'valid_loss'],\n",
       " 'callbacks__pb__pbar': 115/|/ 98%|| 115/117 [04:27<00:00,  3.72it/s, valid_loss=1.58],\n",
       " 'callbacks__lrs': <skorch.callbacks.lr_scheduler.LRScheduler at 0x7f2bf4f2d7b8>,\n",
       " 'callbacks__lrs__policy': 'WarmRestartLR',\n",
       " 'callbacks__lrs__monitor': 'train_loss',\n",
       " 'callbacks__est': <skorch.callbacks.training.EarlyStopping at 0x7f2bf4f2d518>,\n",
       " 'callbacks__est__monitor': 'valid_loss',\n",
       " 'callbacks__est__lower_is_better': True,\n",
       " 'callbacks__est__patience': 5,\n",
       " 'callbacks__est__threshold': 0.0001,\n",
       " 'callbacks__est__threshold_mode': 'rel',\n",
       " 'callbacks__est__sink': <function print>,\n",
       " 'callbacks__print_log': <skorch.callbacks.logging.PrintLog at 0x7f2c0d62fb70>,\n",
       " 'callbacks__print_log__keys_ignored': None,\n",
       " 'callbacks__print_log__sink': <function print>,\n",
       " 'callbacks__print_log__tablefmt': 'simple',\n",
       " 'callbacks__print_log__floatfmt': '.4f',\n",
       " 'callbacks__print_log__stralign': 'right',\n",
       " 'callbacks__pl': <skorch.callbacks.logging.PrintLog at 0x7f2bec6275c0>,\n",
       " 'callbacks__pl__keys_ignored': None,\n",
       " 'callbacks__pl__sink': <function print>,\n",
       " 'callbacks__pl__tablefmt': 'simple',\n",
       " 'callbacks__pl__floatfmt': '.4f',\n",
       " 'callbacks__pl__stralign': 'right'}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.get_params()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = data_loader.indicesMatrix()\n",
    "import sklearn\n",
    "sorted(sklearn.metrics.SCORERS.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_losses(tr_loss, val_loss):\n",
    "    plt.plot(tr_loss, label=\"training\")\n",
    "    plt.plot(val_loss, label=\"validation\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo plot losses\n",
    "#todo plot accuracy\n",
    "#todo confusion matrix\n",
    "history = net.history\n",
    "train_losses = history[:, 'train_loss']\n",
    "valid_losses = history[:, 'valid_loss']\n",
    "\n",
    "accuracy = history[:, 'accuracy']\n",
    "plot_losses(train_losses, valid_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.tensor(weights_matrix).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Variable(w.double())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
