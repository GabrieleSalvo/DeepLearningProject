{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from base import BaseDataLoader\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import random\n",
    "from gensim.parsing.preprocessing import remove_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch import NeuralNetClassifier\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmznDataLoader():\n",
    "    def __init__(self):\n",
    "        df = self.getDF('./data/reviews_Amazon_Instant_Video_5.json.gz')\n",
    "#         display(df)\n",
    "#         print(self.df.loc[0][['overall','reviewText']])\n",
    "        df = df[['reviewText', 'overall']]\n",
    "        df['reviewText'] = df['reviewText'].apply(lambda x : self.title_parsing(x))\n",
    "        X = df['reviewText']\n",
    "        y = df['overall']-1\n",
    "        self.weight_matrix = self.get_weight_matrix(X)\n",
    "        _, self.idx2word = self.buildCorpus(X)\n",
    "        X = self.indicesMatrix(X)\n",
    "        self.data_features = X\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    def get_attention_mask(self):\n",
    "        index_matrix = data_loader.data_features\n",
    "        mask = (index_matrix!=0)\n",
    "        mask[:,-1] = False\n",
    "        attention_weigths = np.random.random(index_matrix.shape).round(1) * 2 + 3\n",
    "        attention_weigths[~mask] = float('-inf')\n",
    "        attention_weigths = softmax(attention_weigths, axis=1)\n",
    "        return attention_weigths\n",
    "\n",
    "    def title_parsing(self, title):  \n",
    "        # remove stop words and tokenization \n",
    "        title = re.sub('[^a-zA-Z]', ' ', str(title))\n",
    "        title = title.lower()\n",
    "        title = remove_stopwords(title)  ## remove stop words, corpus size 52680            \n",
    "        title = title.split()\n",
    "        title = [word for word in title if len(word) >1 ]\n",
    "        return title\n",
    "\n",
    "    def parse(self, path):\n",
    "        g = gzip.open(path, 'rb')\n",
    "        for l in g:\n",
    "            yield eval(l)\n",
    "\n",
    "    def getDF(self, path):\n",
    "        i = 0\n",
    "        df = {}\n",
    "        for d in self.parse(path):\n",
    "            df[i] = d\n",
    "            i += 1\n",
    "        return pd.DataFrame.from_dict(df, orient='index')        \n",
    "\n",
    "    def buildCorpus(self, X):\n",
    "        '''\n",
    "        return a dictionary with 'word' and its index in corpus as key and value respectively\n",
    "        '''\n",
    "        word2idx = {}\n",
    "        idx2word = [] ## alternatively use if.. condition\n",
    "        idx = 0 \n",
    "        for row in X:\n",
    "            for word in row:\n",
    "                if word not in word2idx:\n",
    "                    idx2word.append(word)                \n",
    "                    word2idx[word] = len(idx2word) - 1\n",
    "#             pickle.dump(word2idx, open('./data/corpusDict.txt', 'wb'))\n",
    "        return word2idx, idx2word  \n",
    "\n",
    "\n",
    "    def indicesMatrix(self, X):\n",
    "        '''\n",
    "        return matrix (num_reviews, maxNumberWords) such that review text transformed to index\n",
    "        '''\n",
    "        word2idx, _ = self.buildCorpus(X)\n",
    "#             word2idx = pickle.load(open('./data/corpusDict.txt', 'rb'))\n",
    "        ## 53008 words in corpus\n",
    "\n",
    "        corpusSize = len(word2idx) \n",
    "\n",
    "        maxNumberWords = sorted(len(x) for x in X)[-1]\n",
    "#             print (\"maximum\", maxNumberWords)\n",
    "\n",
    "        index_matrix = np.zeros((X.shape[0], maxNumberWords))          \n",
    "        for i, row in enumerate(X):\n",
    "            for j, word in enumerate(row):\n",
    "#                 try:\n",
    "#                     index_matrix[i,j] = word2idx[word]\n",
    "#                     words_found += 1\n",
    "#                 except KeyError:\n",
    "#                     index_matrix[i,j] = corpusSize     \n",
    "                index_matrix[i,j] = word2idx[word]\n",
    "        if maxNumberWords % 2 == 1:\n",
    "            x0 = np.full((index_matrix.shape[0], 1), maxNumberWords)\n",
    "            index_matrix = np.hstack((index_matrix, x0))\n",
    "        return index_matrix\n",
    "\n",
    "    def get_weight_matrix(self, X):\n",
    "        '''\n",
    "        return matrix contains embedding for word in corpus/review text\n",
    "        Note that the word cannot be found in the glove returns ?? as embedding\n",
    "        '''\n",
    "\n",
    "        glove = {}\n",
    "\n",
    "        with open(f'./data/glove.6B.50d.txt', 'rb') as f:\n",
    "            for l in f:\n",
    "                line = l.decode().split()           \n",
    "                word = line[0]\n",
    "    #            words.append(word)\n",
    "    #             word2idx[word] = idx\n",
    "    #            idx += 1\n",
    "                vect = np.array(line[1:]).astype(np.float)\n",
    "    #             vectors.append(vect)\n",
    "    #     vectors = np.reshape(vectors, (400000, 50))\n",
    "                glove.update({word:vect})\n",
    "    #     glove = {word2idx[w]: vectors[word2idx[w]] for w in words}  # alternatively generate weights_matrix directly\n",
    "\n",
    "        target_vocab, _ = self.buildCorpus(X)\n",
    "        #except\n",
    "        #exceptKey = list(set(list(glove.keys())).difference(list(target_vocab.keys())))  ## \n",
    "        matrix_len = len(target_vocab)\n",
    "        weights_matrix = np.zeros((matrix_len, 50))\n",
    "        words_found = 0\n",
    "        words_not_found = 0\n",
    "        for i, word in enumerate(target_vocab):\n",
    "            try: \n",
    "                weights_matrix[i] = glove[word]\n",
    "                words_found += 1\n",
    "            except KeyError:\n",
    "                words_not_found += 1\n",
    "                weights_matrix[i] = np.random.normal(scale=0.6, size=(50,))\n",
    "\n",
    "        print(words_not_found)\n",
    "        return  weights_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9776\n"
     ]
    }
   ],
   "source": [
    "data_loader = AmznDataLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>helpful</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A11N155CW1UV02</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>AdrianaM</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>I had big expectations because I love English ...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>A little bit boring for me</td>\n",
       "      <td>1399075200</td>\n",
       "      <td>05 3, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A3BC8O2KCL29V2</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>Carol T</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>I highly recommend this series. It is a must f...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Excellent Grown Up TV</td>\n",
       "      <td>1346630400</td>\n",
       "      <td>09 3, 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A60D5HQFOTSOM</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>Daniel Cooper \"dancoopermedia\"</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>This one is a real snoozer. Don't believe anyt...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Way too boring for me</td>\n",
       "      <td>1381881600</td>\n",
       "      <td>10 16, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A1RJPIGRSNX4PW</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>J. Kaplan \"JJ\"</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>Mysteries are interesting.  The tension betwee...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Robson Green is mesmerizing</td>\n",
       "      <td>1383091200</td>\n",
       "      <td>10 30, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A16XRPF40679KG</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>Michael Dobey</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>This show always is excellent, as far as briti...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Robson green and great writing</td>\n",
       "      <td>1234310400</td>\n",
       "      <td>02 11, 2009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID        asin                    reviewerName helpful  \\\n",
       "0  A11N155CW1UV02  B000H00VBQ                        AdrianaM  [0, 0]   \n",
       "1  A3BC8O2KCL29V2  B000H00VBQ                         Carol T  [0, 0]   \n",
       "2   A60D5HQFOTSOM  B000H00VBQ  Daniel Cooper \"dancoopermedia\"  [0, 1]   \n",
       "3  A1RJPIGRSNX4PW  B000H00VBQ                  J. Kaplan \"JJ\"  [0, 0]   \n",
       "4  A16XRPF40679KG  B000H00VBQ                   Michael Dobey  [1, 1]   \n",
       "\n",
       "                                          reviewText  overall  \\\n",
       "0  I had big expectations because I love English ...      2.0   \n",
       "1  I highly recommend this series. It is a must f...      5.0   \n",
       "2  This one is a real snoozer. Don't believe anyt...      1.0   \n",
       "3  Mysteries are interesting.  The tension betwee...      4.0   \n",
       "4  This show always is excellent, as far as briti...      5.0   \n",
       "\n",
       "                          summary  unixReviewTime   reviewTime  \n",
       "0      A little bit boring for me      1399075200   05 3, 2014  \n",
       "1           Excellent Grown Up TV      1346630400   09 3, 2012  \n",
       "2           Way too boring for me      1381881600  10 16, 2013  \n",
       "3     Robson Green is mesmerizing      1383091200  10 30, 2013  \n",
       "4  Robson green and great writing      1234310400  02 11, 2009  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = data_loader.getDF('./data/reviews_Amazon_Instant_Video_5.json.gz')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "data_loader.idx2word\n",
    "corpus = os.linesep.join(data_loader.idx2word)\n",
    "print(type(corpus))\n",
    "with open(\"text_for_bert.txt\", \"w\", encoding='utf-8') as fp:\n",
    "    fp.write(corpus)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02755311660771048"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(data_loader.X_train)/(data_loader.X_train.shape[0]*data_loader.X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29700, 1502)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader.X_train.shape\n",
    "# data_loader.buildCorpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.086395263671875\n"
     ]
    }
   ],
   "source": [
    "weight_matrix = data_loader.weight_matrix\n",
    "import sys\n",
    "print(sys.getsizeof(weight_matrix)/(2**20))\n",
    "\n",
    "# print(weight_matrix[0])\n",
    "# print(weight_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(weight_matrix,  open('./data/GloveMatrix.npy', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo reset parameters for linear layer\n",
    "class ReviewModel(nn.Module):\n",
    "    def __init__(self, max_length, batch_size):\n",
    "        super(ReviewModel, self).__init__()\n",
    "        weights_matrix = data_loader.weight_matrix\n",
    "        self.max_length = max_length\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.tensor(weights_matrix), freeze=False)\n",
    "#        self.embedding = nn.Embedding(weights_matrix.shape[0],weights_matrix.shape[1])\n",
    "#         self.embedding.weight.data.copy_(torch.from_numpy(weights_matrix))\n",
    "        self.drop1 = nn.Dropout(p=0.5)\n",
    "        self.conv1 = nn.Conv1d(in_channels=50, out_channels=200, kernel_size=4, padding=2).double()\n",
    "        self.conv2 = nn.Conv1d(in_channels=50, out_channels=200, kernel_size=5, padding=2).double()\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=2)\n",
    "        self.drop2 = nn.Dropout(p=0.15)\n",
    "        \n",
    "        self.rnn = nn.GRU(input_size=400, hidden_size=100, num_layers=1, batch_first=False)  #\n",
    "#        self.fc1 =nn.Linear(in_features=1501*100, out_features = 400)\n",
    "        self.fc1 =nn.Linear(in_features=(max_length//2)*100, out_features = 400)\n",
    "        self.drop3 = nn.Dropout(p=0.1)\n",
    "        self.fc2 = nn.Linear(in_features=400, out_features=5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "#         print(\"input tensor\", x.size())\n",
    "        x = self.embedding(x)\n",
    "#         print(\"after embedding\", x.size())\n",
    "        x = self.drop1(x)\n",
    "#        print(x)\n",
    "        x = x.view(-1,50,self.max_length)  # input(N,C_in,L) to conv1d\n",
    "        x1 = F.relu(self.conv1(x))\n",
    "#         print(\"x1 shape\", x1.size())\n",
    "        x2 = F.relu(self.conv2(x))\n",
    "#         print(\"x2 shape \", x2.size())\n",
    "\n",
    "        x1 = self.maxpool(x1)\n",
    "#         print(\"x1 shape\", x1.size())        \n",
    "        x2 = self.maxpool(x2)\n",
    "#         print(\"x2 shape\", x2.size())        \n",
    "        x = torch.cat((x1,x2), 1)\n",
    "        x = self.drop2(x)\n",
    "        \n",
    "        x = x.view((self.max_length//2),-1, 400).float()#### change dimensionality\n",
    "        \n",
    "        hidden = Variable(torch.cuda.FloatTensor(1, batch_size, 100).uniform_()) \n",
    "        \n",
    "#        print(\"before rnn \", x.size(), \"hidden size \",hidden.size())\n",
    "\n",
    "        output, _ = self.rnn(x,hidden)\n",
    "#         print(\"AFTER rnn \",output.size())\n",
    "        \n",
    "        x = output.contiguous().view(-1, (self.max_length//2)*100)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.drop3(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "#     def create_emb_layer(self, weights_matrix, non_trainable=True):\n",
    "#         num_embeddings, embedding_dim = weights_matrix.shape\n",
    "#         emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
    "#         emb_layer.load_state_dict({'weight': weights_matrix})\n",
    "#         if non_trainable:\n",
    "#             emb_layer.weight.requires_grad = False\n",
    "\n",
    "#         return emb_layer, num_embeddings, embedding_dim\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pytorch_pretrained_bert import BertTokenizer, BertModel\n",
    "# from flair.embeddings import BertEmbeddings\n",
    "# bert_embedding = BertEmbeddings('bert-base-uncased')\n",
    "# from flair.data import Sentence\n",
    "# sentence = Sentence('hello world')\n",
    "# emb = bert_embedding.embed(sentence)\n",
    "# for token in emb:\n",
    "#     print(token.embedding.detach().cpu().numpy().shape)\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# model = BertModel.from_pretrained('bert-base-uncased')\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewModelwithBert(nn.Module):\n",
    "    def __init__(self, max_length, batch_size, weights_matrix):\n",
    "        super(ReviewModelwithBert, self).__init__()\n",
    "#         weights_matrix = data_loader.weight_matrix\n",
    "        self.max_length = max_length\n",
    "        self.batch_size = batch_size\n",
    "#         self.embedding = nn.Embedding.from_pretrained(torch.tensor(weights_matrix), freeze=False)\n",
    "        vocab_size =  weights_matrix.shape[0]\n",
    "        embedding_dim = weights_matrix.shape[1]\n",
    "        self.embedding = nn.Embedding(weights_matrix.shape[0],weights_matrix.shape[1]).double()\n",
    "#         self.embedding.weight.data.copy_(torch.from_numpy(weights_matrix))\n",
    "        self.drop1 = nn.Dropout(p=0.5)\n",
    "        self.conv1 = nn.Conv1d(in_channels=50, out_channels=200, kernel_size=4, padding=2).double()\n",
    "        self.conv2 = nn.Conv1d(in_channels=50, out_channels=200, kernel_size=5, padding=2).double()\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=2)\n",
    "        self.drop2 = nn.Dropout(p=0.15)\n",
    "        \n",
    "        self.rnn = nn.GRU(input_size=400, hidden_size=100, num_layers=1, batch_first=False)  #\n",
    "#        self.fc1 =nn.Linear(in_features=1501*100, out_features = 400)\n",
    "        self.fc1 =nn.Linear(in_features=(max_length//2)*100, out_features = 400)\n",
    "        self.drop3 = nn.Dropout(p=0.1)\n",
    "        self.fc2 = nn.Linear(in_features=400, out_features=5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "#         print(\"input tensor\", x.size())\n",
    "#         attn_weigths = F.softmax(x, dim=1)\n",
    "        x = self.embedding(x)\n",
    "#         print(\"after embedding\", x.size())\n",
    "        \n",
    "        x = self.drop1(x)\n",
    "#        print(x)\n",
    "        x = x.view(-1,50,self.max_length)  # input(N,C_in,L) to conv1d\n",
    "        x1 = F.relu(self.conv1(x))\n",
    "#         print(\"x1 shape\", x1.size())\n",
    "        x2 = F.relu(self.conv2(x))\n",
    "#         print(\"x2 shape \", x2.size())\n",
    "\n",
    "        x1 = self.maxpool(x1)\n",
    "#         print(\"x1 shape\", x1.size())        \n",
    "        x2 = self.maxpool(x2)\n",
    "#         print(\"x2 shape\", x2.size())        \n",
    "        x = torch.cat((x1,x2), 1)\n",
    "        x = self.drop2(x)\n",
    "        \n",
    "        x = x.view((self.max_length//2),-1, 400).float()#### change dimensionality\n",
    "        \n",
    "        hidden = Variable(torch.cuda.FloatTensor(1, batch_size, 100).uniform_()) \n",
    "        \n",
    "#        print(\"before rnn \", x.size(), \"hidden size \",hidden.size())\n",
    "\n",
    "        output, _ = self.rnn(x,hidden)\n",
    "#         print(\"AFTER rnn \",output.size())\n",
    "        \n",
    "        x = output.contiguous().view(-1, (self.max_length//2)*100)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.drop3(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def get_tensors(self, word):\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        # Tokenized input\n",
    "        tokenized_text = tokenizer.tokenize(word)\n",
    "\n",
    "        # Mask a token that we will try to predict back with `BertForMaskedLM`\n",
    "\n",
    "        # Convert token to vocabulary indices\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "        # Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\n",
    "        # print(len(tokenized_text))\n",
    "        tokens = []\n",
    "        input_type_ids = []\n",
    "        tokens.append(\"[CLS]\")\n",
    "        input_type_ids.append(0)\n",
    "        for token in tokenized_text:\n",
    "            tokens.append(token)\n",
    "            input_type_ids.append(0)\n",
    "        tokens.append(\"[SEP]\")\n",
    "        input_type_ids.append(0)\n",
    "\n",
    "        input_mask = [1] * len(input_ids)\n",
    "        seq_length  = 512\n",
    "        # print('input_ids ', len(input_ids))\n",
    "        # print('input_mask ', len(input_mask))\n",
    "        # print('input_type_ids ', len(input_type_ids))\n",
    "        # Zero-pad up to the sequence length.\n",
    "        while len(input_ids) < seq_length:\n",
    "            input_ids.append(0)\n",
    "        while len(input_mask) < seq_length:\n",
    "            input_mask.append(0)\n",
    "        while len(input_type_ids)  < seq_length:\n",
    "            input_type_ids.append(0)\n",
    "        assert len(input_ids) == seq_length\n",
    "        assert len(input_mask) == seq_length\n",
    "        assert len(input_type_ids) == seq_length, len(input_type_ids)\n",
    "        # Convert inputs to PyTorch tensors\n",
    "        input_ids = torch.tensor([input_ids])\n",
    "        input_mask = torch.tensor([input_mask])\n",
    "        input_type_ids = torch.tensor([input_type_ids])\n",
    "        return input_ids, input_mask, input_type_ids\n",
    "    \n",
    "    \n",
    "    def get_bert_embedding(self, word, model):\n",
    "        input_ids, input_mask, input_type_ids = self.get_tensors(word)\n",
    "\n",
    "#         print(type(indexed_tokens))\n",
    "#         for _ in range(510-len(indexed_tokens)):\n",
    "#             indexed_tokens.append(0)\n",
    "#         segments_tensors = np.zeros((510))\n",
    "#         non_zeros = np.count_nonzero(indexed_tokens)\n",
    "#         segments_tensors[:non_zeros] = 1\n",
    "#         indexed_tokens = torch.tensor(indexed_tokens).long()\n",
    "#         segments_tensors = torch.tensor(segments_tensors).long()\n",
    "\n",
    "#         print(indexed_tokens.size())\n",
    "#         print(segments_tensors.size())\n",
    "        encoded_layers, pooled_output = model(input_ids, input_type_ids, input_mask)\n",
    "#         print(encoded_layers, pooled_output)\n",
    "        return pooled_output\n",
    "    def embedding_bert(self, x, seq_length, embedding_size, model):\n",
    "        #batch_size, seq-length\n",
    "        #transofrm into batch_size, seq_length, embedding size\n",
    "        #idx2word search\n",
    "        emb = torch.zeros(batch_size, seq_length, embedding_size)\n",
    "        for i in range(batch_size):\n",
    "            phrase_length = np.count_nonzero( x[i,:].detach().cpu().numpy())-1\n",
    "            for j in range(phrase_length):\n",
    "                word = data_loader.idx2word[x[i,j].detach().cpu().numpy().astype(np.int)]\n",
    "                emb[i, j,:] = self.get_bert_embedding(word, model)\n",
    "        return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch.callbacks import EpochScoring, PrintLog, ProgressBar, LRScheduler, EarlyStopping\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ds_accuracy(net, ds, y=None):\n",
    "    # assume ds yields (X, y), e.g. torchvision.datasets.MNIST\n",
    "    y_true = [y for _, y in ds]\n",
    "    y_pred = net.predict(ds)\n",
    "    return sklearn.metrics.accuracy_score(y_true, y_pred)\n",
    "\n",
    "\n",
    "callbacks = [\n",
    "    ('lrs',LRScheduler()),\n",
    "    ('est',EarlyStopping()) \n",
    "]\n",
    "\n",
    "batch_size = 256\n",
    "device = torch.device(\"cuda:0\")\n",
    "model = ReviewModelwithBert(data_loader.X_train.shape[1], batch_size, data_loader.weight_matrix).to(device)\n",
    "net = NeuralNetClassifier(model,## change dimensionality\n",
    "                          iterator_train__drop_last = True,\n",
    "                          iterator_valid__drop_last = True,\n",
    "                          iterator_train__shuffle = True,\n",
    "                          iterator_valid__shuffle = True,\n",
    "                          max_epochs=20, \n",
    "                          lr=0.01, \n",
    "                          criterion = nn.CrossEntropyLoss, \n",
    "                          optimizer=optim.Adam,\n",
    "                          batch_size = batch_size,\n",
    "                          callbacks = callbacks,\n",
    "                          device = torch.device('cuda:0')\n",
    "                          )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss       dur\n",
      "-------  ------------  -----------  ------------  --------\n",
      "      1      \u001b[36m237.4966\u001b[0m       \u001b[32m0.5617\u001b[0m        \u001b[35m1.2024\u001b[0m  195.9338\n",
      "      2        \u001b[36m1.2023\u001b[0m       0.5613        \u001b[35m1.2019\u001b[0m  195.7959\n",
      "      3        1.2029       0.5615        1.2032  196.6091\n",
      "      4        \u001b[36m1.2022\u001b[0m       \u001b[32m0.5620\u001b[0m        1.2037  195.8881\n",
      "      5        \u001b[36m1.2019\u001b[0m       \u001b[32m0.5627\u001b[0m        1.2019  195.7805\n",
      "      6        1.2025       0.5608        1.2043  195.9226\n",
      "      7        1.2019       0.5623        \u001b[35m1.2017\u001b[0m  196.7184\n",
      "      8        1.2020       \u001b[32m0.5628\u001b[0m        \u001b[35m1.2006\u001b[0m  197.0827\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.from_numpy(data_loader.X_train).long().to(device)\n",
    "labels = torch.tensor(data_loader.y_train.values).long().to(device)\n",
    "\n",
    "# rest = inputs.size()[0] % batch_size\n",
    "# print(rest)\n",
    "# print(inputs.size())\n",
    "# inputs = inputs[:-rest]\n",
    "# print(inputs.size()[0]/batch_size)\n",
    "# print(inputs.shape)\n",
    "# labels = labels[:-rest]\n",
    "\n",
    "net.fit(inputs, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.get_params()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = data_loader.indicesMatrix()\n",
    "import sklearn\n",
    "sorted(sklearn.metrics.SCORERS.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_losses(tr_loss, val_loss):\n",
    "    plt.plot(tr_loss, label=\"training\")\n",
    "    plt.plot(val_loss, label=\"validation\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo plot losses\n",
    "#todo plot accuracy\n",
    "#todo confusion matrix\n",
    "history = net.history\n",
    "train_losses = history[:, 'train_loss']\n",
    "valid_losses = history[:, 'valid_loss']\n",
    "\n",
    "accuracy = history[:, 'accuracy']\n",
    "plot_losses(train_losses, valid_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.tensor(weights_matrix).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Variable(w.double())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
