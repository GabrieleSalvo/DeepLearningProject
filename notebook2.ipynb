{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/modules/Ubuntu/14.04/amd64/common/anaconda3/latest/lib/python3.7/site-packages/smart_open/ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from base import BaseDataLoader\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import visdom\n",
    "import random\n",
    "from gensim.parsing.preprocessing import remove_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch import NeuralNetClassifier\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmznDataLoader():\n",
    "        def __init__(self):\n",
    "            df = self.getDF('./data/reviews_Amazon_Instant_Video_5.json.gz')\n",
    "    #         display(self.df)\n",
    "    #         print(self.df.loc[0][['overall','reviewText']])\n",
    "            df = df[['reviewText', 'overall']]\n",
    "            df['reviewText'] = df['reviewText'].apply(lambda x : self.title_parsing(x))\n",
    "            self.X = df['reviewText']\n",
    "            self.y = df['overall']-1\n",
    "            self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X, self.y, test_size=0.2, random_state=42)\n",
    "            \n",
    "        def title_parsing(self, title):  \n",
    "            # remove stop words and tokenization \n",
    "            title = re.sub('[^a-zA-Z]', ' ', str(title))\n",
    "            title = title.lower()\n",
    "            #title = remove_stopwords(title)  ## remove stop words, corpus size 52680            \n",
    "            title = title.split()\n",
    "            title = [word for word in title if len(word) >1 ]\n",
    "            return title\n",
    "        \n",
    "        \n",
    "        def parse(self, path):\n",
    "            g = gzip.open(path, 'rb')\n",
    "            for l in g:\n",
    "                yield eval(l)\n",
    "            \n",
    "        def getDF(self, path):\n",
    "            i = 0\n",
    "            df = {}\n",
    "            for d in self.parse(path):\n",
    "                df[i] = d\n",
    "                i += 1\n",
    "            return pd.DataFrame.from_dict(df, orient='index')        \n",
    "\n",
    "        def buildCorpus(self):\n",
    "            '''\n",
    "            return a dictionary with 'word' and its index in corpus as key and value respectively\n",
    "            '''\n",
    "            word2idx = {}\n",
    "            idx2word = [] ## alternatively use if.. condition\n",
    "            idx = 0 \n",
    "            for row in self.X:\n",
    "                for word in row:\n",
    "                    if word not in word2idx:\n",
    "                        idx2word.append(word)                \n",
    "                        word2idx[word] = len(idx2word) - 1\n",
    "#             pickle.dump(word2idx, open('./data/corpusDict.txt', 'wb'))\n",
    "            return word2idx   \n",
    "        \n",
    "    \n",
    "        def indicesMatrix(self):\n",
    "            '''\n",
    "            return matrix such that review text transformed to index\n",
    "            '''\n",
    "            word2idx = self.buildCorpus()\n",
    "#             word2idx = pickle.load(open('./data/corpusDict.txt', 'rb'))\n",
    "            ## 53008 words in corpus\n",
    "    \n",
    "            corpusSize = len(word2idx) \n",
    "        \n",
    "            maxNumberWords = sorted(len(x) for x in self.X)[-1]\n",
    "            print (\"maximum\", maxNumberWords)\n",
    "\n",
    "            index_matrix = np.zeros((self.X.shape[0], maxNumberWords))          \n",
    "            for i, row in enumerate(self.X):\n",
    "                for j, word in enumerate(row):\n",
    "#                     try:\n",
    "#                         index_matrix[i,j] = word2idx[word]\n",
    "#                         words_found += 1\n",
    "#                     except KeyError:\n",
    "#                         index_matrix[i,j] = corpusSize     \n",
    "\n",
    "                    index_matrix[i,j] = word2idx[word]\n",
    "            return index_matrix\n",
    "        \n",
    "\n",
    "        #             self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = AmznDataLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum 2937\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.000e+00, 1.000e+00, 2.000e+00, ..., 0.000e+00, 0.000e+00,\n",
       "        0.000e+00],\n",
       "       [2.600e+01, 2.700e+01, 1.400e+01, ..., 0.000e+00, 0.000e+00,\n",
       "        0.000e+00],\n",
       "       [1.400e+01, 4.200e+01, 1.600e+01, ..., 0.000e+00, 0.000e+00,\n",
       "        0.000e+00],\n",
       "       ...,\n",
       "       [7.500e+01, 4.900e+01, 1.067e+03, ..., 0.000e+00, 0.000e+00,\n",
       "        0.000e+00],\n",
       "       [1.280e+02, 4.200e+01, 3.630e+02, ..., 0.000e+00, 0.000e+00,\n",
       "        0.000e+00],\n",
       "       [2.030e+02, 6.100e+01, 4.870e+03, ..., 0.000e+00, 0.000e+00,\n",
       "        0.000e+00]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader.indicesMatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GloveMatrix():\n",
    "    '''\n",
    "    return matrix contains embedding for word in corpus/review text\n",
    "    Note that the word cannot be found in the glove returns ?? as embedding\n",
    "    '''\n",
    "    \n",
    "    ## load glove files into dictionary \n",
    "#     words = []\n",
    "#     idx = 0\n",
    "#     word2idx = {}\n",
    "    # vectors = bcolz.carray(np.zeros(1), rootdir=f'./data/glove_6B_50.dat', mode='w')\n",
    "#     vectors = []\n",
    "    glove = {}\n",
    "    \n",
    "    with open(f'./data/glove.6B.50d.txt', 'rb') as f:\n",
    "        for l in f:\n",
    "            line = l.decode().split()           \n",
    "            word = line[0]\n",
    "#            words.append(word)\n",
    "#             word2idx[word] = idx\n",
    "#            idx += 1\n",
    "            vect = np.array(line[1:]).astype(np.float)\n",
    "#             vectors.append(vect)\n",
    "#     vectors = np.reshape(vectors, (400000, 50))\n",
    "            glove.update({word:vect})\n",
    "#     glove = {word2idx[w]: vectors[word2idx[w]] for w in words}  # alternatively generate weights_matrix directly\n",
    "\n",
    "    target_vocab = data_loader.buildCorpus()\n",
    "    #except\n",
    "    #exceptKey = list(set(list(glove.keys())).difference(list(target_vocab.keys())))  ## \n",
    "    matrix_len = len(target_vocab)\n",
    "    weights_matrix = np.zeros((matrix_len, 50))\n",
    "    words_found = 0\n",
    "    words_not_found = 0\n",
    "    for i, word in enumerate(target_vocab):\n",
    "        try: \n",
    "            weights_matrix[i] = glove[word]\n",
    "            words_found += 1\n",
    "        except KeyError:\n",
    "            words_not_found += 1\n",
    "            weights_matrix[i] = np.random.normal(scale=0.6, size=(50,))\n",
    "    print(words_not_found)\n",
    "    return  weights_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9777\n",
      "[ 0.60348  -0.52096   0.40851  -0.37217   0.36978   0.61082  -1.3228\n",
      "  0.24375  -0.5942   -0.35708   0.39942   0.031911 -1.0643   -0.52327\n",
      "  0.71453   0.063384 -0.46383  -0.34641  -0.72445  -0.13714  -0.19179\n",
      "  0.72225   0.6295   -0.8086   -0.037694 -2.0355    0.10566  -0.038591\n",
      " -0.23201  -0.29627   3.3215    0.032443  0.085368 -0.40771   0.45341\n",
      " -0.099674  0.44704   0.5422    0.18185   0.17504  -0.33833   0.31697\n",
      " -0.025268  0.095795 -0.25071  -0.47564  -1.0407   -0.15138  -0.22057\n",
      " -0.59633 ]\n",
      "(52982, 50)\n"
     ]
    }
   ],
   "source": [
    "weight_matrix = GloveMatrix()\n",
    "print(weight_matrix[0])\n",
    "print(weight_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(weight_matrix,  open('./data/GloveMatrix.npy', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum 2937\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(37126, 2937)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader.indicesMatrix().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo reset parameters for linear layer\n",
    "class FirstModel(nn.Module):\n",
    "    def __init__(self, max_length):\n",
    "        super(FirstModel, self).__init__()\n",
    "        weights_matrix = GloveMatrix()\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.tensor(weights_matrix), freeze=False)\n",
    "#        self.embedding = nn.Embedding(weights_matrix.shape[0],weights_matrix.shape[1])\n",
    "#         self.embedding.weight.data.copy_(torch.from_numpy(weights_matrix))\n",
    "        self.drop1 = nn.Dropout(p=0.5)\n",
    "        self.conv1 = nn.Conv1d(in_channels=50, out_channels=200, kernel_size=4, padding=1).double()\n",
    "        self.conv2 = nn.Conv1d(in_channels=50, out_channels=200, kernel_size=5, padding=1).double()\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=2)\n",
    "        self.drop2 = nn.Dropout(p=0.15)\n",
    "        self.hx = Variable(torch.randn(1, 256, 100))\n",
    "        self.rnn = nn.GRU(input_size=400, hidden_size=100, num_layers=1, batch_first=True)  \n",
    "#        self.fc1 =nn.Linear(in_features=1501*100, out_features = 400)\n",
    "        self.fc1 =nn.Linear(in_features=1534*100, out_features = 400)\n",
    "        self.drop3 = nn.Dropout(p=0.1)\n",
    "        self.fc2 = nn.Linear(in_features=400, out_features=10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        print(\"input tensor\", x.size())\n",
    "        x = self.embedding(x)\n",
    "        print(\"after embedding\", x.size())\n",
    "        x = self.drop1(x)\n",
    "#        print(x)\n",
    "\n",
    "        x = x.view(256,50,-1)  # input(N,C_in,L) to conv1d\n",
    "        x1 = F.relu(self.conv1(x))\n",
    "        print(\"x1 shape\", x1.size())\n",
    "        x2 = F.relu(self.conv2(x))\n",
    "        print(\"x2 shape \", x2.size())\n",
    "\n",
    "        x1 = self.maxpool(x1)\n",
    "        print(\"x1 shape\", x1.size())        \n",
    "        x2 = self.maxpool(x2)\n",
    "        print(\"x2 shape\", x2.size())        \n",
    "        x = torch.cat((x1,x2), 2)\n",
    "        x = self.drop2(x)\n",
    "        #x = pack_padded_sequence(x, lengths, batch_first=True)\n",
    "#        x = x.view(256,1501,400)\n",
    "        x = x.view(256, -1, 400).float()#### change dimensionality\n",
    "        output, hidden = self.rnn(x, self.hx)\n",
    "#         for i in range(10):\n",
    "#             self.hx = self.rnn(x[i], self.hx)\n",
    "#             output.append(self.hx)\n",
    "        x = output.contiguous().view(256,-1)\n",
    "        print(x.size())\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.drop3(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "#     def create_emb_layer(self, weights_matrix, non_trainable=True):\n",
    "#         num_embeddings, embedding_dim = weights_matrix.shape\n",
    "#         emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
    "#         emb_layer.load_state_dict({'weight': weights_matrix})\n",
    "#         if non_trainable:\n",
    "#             emb_layer.weight.requires_grad = False\n",
    "\n",
    "#         return emb_layer, num_embeddings, embedding_dim\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum 2937\n"
     ]
    }
   ],
   "source": [
    "net = NeuralNetClassifier(FirstModel,\n",
    "                          module__max_length=data_loader.indicesMatrix().shape[1],\n",
    "                          max_epochs=5, \n",
    "                          lr=0.001, \n",
    "                          criterion = nn.CrossEntropyLoss, \n",
    "                          optimizer=optim.SGD,\n",
    "                          optimizer__param_groups=[('momentum', 0.9)],\n",
    "                          batch_size = 256\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum 2937\n",
      "9777\n",
      "input tensor torch.Size([256, 2937])\n",
      "after embedding torch.Size([256, 2937, 50])\n",
      "x1 shape torch.Size([256, 200, 2936])\n",
      "x2 shape  torch.Size([256, 200, 2935])\n",
      "x1 shape torch.Size([256, 200, 1468])\n",
      "x2 shape torch.Size([256, 200, 1467])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.from_numpy(data_loader.indicesMatrix()).long()\n",
    "labels = torch.tensor(data_loader.y)\n",
    "net.fit(inputs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = data_loader.indicesMatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.tensor(data_loader.y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.tensor(weights_matrix).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Variable(w.double())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
