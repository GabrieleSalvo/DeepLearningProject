{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from base import BaseDataLoader\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import visdom\n",
    "import random\n",
    "from gensim.parsing.preprocessing import remove_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch import NeuralNetClassifier\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmznDataLoader():\n",
    "        def __init__(self):\n",
    "            df = self.getDF('./data/reviews_Amazon_Instant_Video_5.json.gz')\n",
    "    #         display(self.df)\n",
    "    #         print(self.df.loc[0][['overall','reviewText']])\n",
    "            df = df[['reviewText', 'overall']]\n",
    "            df['reviewText'] = df['reviewText'].apply(lambda x : self.title_parsing(x))\n",
    "            X = df['reviewText']\n",
    "            y = df['overall']\n",
    "            self.weight_matrix = self.get_weigth_matrix(X)\n",
    "            X = self.indicesMatrix(X)\n",
    "            self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "            \n",
    "            \n",
    "        def title_parsing(self, title):  \n",
    "            # remove stop words and tokenization \n",
    "            title = re.sub('[^a-zA-Z]', ' ', str(title))\n",
    "            title = title.lower()\n",
    "            title = remove_stopwords(title)  ## remove stop words, corpus size 52680            \n",
    "            title = title.split()\n",
    "            title = [word for word in title if len(word) >1 ]\n",
    "            return title\n",
    "        \n",
    "        \n",
    "        def parse(self, path):\n",
    "            g = gzip.open(path, 'rb')\n",
    "            for l in g:\n",
    "                yield eval(l)\n",
    "            \n",
    "        def getDF(self, path):\n",
    "            i = 0\n",
    "            df = {}\n",
    "            for d in self.parse(path):\n",
    "                df[i] = d\n",
    "                i += 1\n",
    "            return pd.DataFrame.from_dict(df, orient='index')        \n",
    "\n",
    "        def buildCorpus(self, X):\n",
    "            '''\n",
    "            return a dictionary with 'word' and its index in corpus as key and value respectively\n",
    "            '''\n",
    "            word2idx = {}\n",
    "            idx2word = [] ## alternatively use if.. condition\n",
    "            idx = 0 \n",
    "            for row in X:\n",
    "                for word in row:\n",
    "                    if word not in word2idx:\n",
    "                        idx2word.append(word)                \n",
    "                        word2idx[word] = len(idx2word) - 1\n",
    "#             pickle.dump(word2idx, open('./data/corpusDict.txt', 'wb'))\n",
    "            return word2idx   \n",
    "        \n",
    "    \n",
    "        def indicesMatrix(self, X):\n",
    "            '''\n",
    "            return matrix (num_reviews, maxNumberWords) such that review text transformed to index\n",
    "            '''\n",
    "            word2idx = self.buildCorpus(X)\n",
    "#             word2idx = pickle.load(open('./data/corpusDict.txt', 'rb'))\n",
    "            ## 53008 words in corpus\n",
    "    \n",
    "            corpusSize = len(word2idx) \n",
    "        \n",
    "            ###\n",
    "            k = sorted(len(x) for x in X)\n",
    "            plt.hist(k)\n",
    "            ###\n",
    "            \n",
    "#             maxNumberWords = sorted(len(x) for x in X)[-1]\n",
    "#             print (\"maximum\", maxNumberWords)\n",
    "            maxNumberWords =40\n",
    "\n",
    "            index_matrix = np.zeros((X.shape[0], maxNumberWords))          \n",
    "            for i, row in enumerate(X):\n",
    "                for j, word in enumerate(row):\n",
    "#                     try:\n",
    "#                         index_matrix[i,j] = word2idx[word]\n",
    "#                         words_found += 1\n",
    "#                     except KeyError:\n",
    "#                         index_matrix[i,j] = corpusSize     \n",
    "\n",
    "                    index_matrix[i,j] = word2idx[word]\n",
    "                    if j >= maxNumberWords-1 : \n",
    "                        break\n",
    "            if maxNumberWords % 2 == 1:\n",
    "                x0 = np.full((index_matrix.shape[0], 1), maxNumberWords)\n",
    "                index_matrix = np.hstack((index_matrix, x0))\n",
    "            return index_matrix\n",
    "        \n",
    "        def get_weigth_matrix(self, X): #max norm for linear layer\n",
    "            '''\n",
    "            return matrix contains embedding for word in corpus/review text\n",
    "            Note that the word cannot be found in the glove returns ?? as embedding\n",
    "            '''\n",
    "\n",
    "            glove = {}\n",
    "\n",
    "            with open(f'./data/glove.6B.50d.txt', 'rb') as f:\n",
    "                for l in f:\n",
    "                    line = l.decode().split()           \n",
    "                    word = line[0]\n",
    "        #            words.append(word)\n",
    "        #             word2idx[word] = idx\n",
    "        #            idx += 1\n",
    "                    vect = np.array(line[1:]).astype(np.float)\n",
    "        #             vectors.append(vect)\n",
    "        #     vectors = np.reshape(vectors, (400000, 50))\n",
    "                    glove.update({word:vect})\n",
    "        #     glove = {word2idx[w]: vectors[word2idx[w]] for w in words}  # alternatively generate weights_matrix directly\n",
    "\n",
    "            target_vocab = self.buildCorpus(X)\n",
    "            #except\n",
    "            #exceptKey = list(set(list(glove.keys())).difference(list(target_vocab.keys())))  ## \n",
    "            matrix_len = len(target_vocab)\n",
    "            weights_matrix = np.zeros((matrix_len, 50))\n",
    "            words_found = 0\n",
    "            words_not_found = 0\n",
    "            for i, word in enumerate(target_vocab):\n",
    "                try: \n",
    "                    weights_matrix[i] = glove[word]\n",
    "                    words_found += 1\n",
    "                except KeyError:\n",
    "                    words_not_found += 1\n",
    "                    weights_matrix[i] = np.random.normal(scale=0.6, size=(50,))\n",
    "\n",
    "            print(words_not_found)\n",
    "            return  weights_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9776\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_loader = AmznDataLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.551273569023569"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(data_loader.X_train)/(data_loader.X_train.shape[0]*data_loader.X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29700, 40)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader.X_train.shape\n",
    "# data_loader.buildCorpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.086395263671875\n"
     ]
    }
   ],
   "source": [
    "weight_matrix = data_loader.weight_matrix\n",
    "import sys\n",
    "print(sys.getsizeof(weight_matrix)/(2**20))\n",
    "\n",
    "# print(weight_matrix[0])\n",
    "# print(weight_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(weight_matrix,  open('./data/GloveMatrix.npy', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo reset parameters for linear layer\n",
    "class ReviewModel(nn.Module):\n",
    "    def __init__(self, max_length, batch_size):\n",
    "        super(ReviewModel, self).__init__()\n",
    "        weights_matrix = data_loader.weight_matrix\n",
    "        self.max_length = max_length\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.tensor(weights_matrix), freeze=False)\n",
    "#        self.embedding = nn.Embedding(weights_matrix.shape[0],weights_matrix.shape[1])\n",
    "#         self.embedding.weight.data.copy_(torch.from_numpy(weights_matrix))\n",
    "        self.drop1 = nn.Dropout(p=0.5)\n",
    "        self.conv1 = nn.Conv1d(in_channels=50, out_channels=200, kernel_size=4, padding=2).double()\n",
    "        self.conv2 = nn.Conv1d(in_channels=50, out_channels=200, kernel_size=5, padding=2).double()\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=2)\n",
    "        self.drop2 = nn.Dropout(p=0.15)\n",
    "        \n",
    "        self.rnn = nn.GRU(input_size=400, hidden_size=100, num_layers=1, batch_first=False)  #\n",
    "#        self.fc1 =nn.Linear(in_features=1501*100, out_features = 400)\n",
    "        self.fc1 =nn.Linear(in_features=(max_length//2)*100, out_features = 400)\n",
    "        self.drop3 = nn.Dropout(p=0.1)\n",
    "        self.fc2 = nn.Linear(in_features=400, out_features=5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "#         print(\"input tensor\", x.size())\n",
    "        x = self.embedding(x)\n",
    "#         print(\"after embedding\", x.size())\n",
    "        x = self.drop1(x)\n",
    "#        print(x)\n",
    "        x = x.view(-1,50,self.max_length)  # input(N,C_in,L) to conv1d\n",
    "        x1 = F.relu(self.conv1(x))\n",
    "#         print(\"x1 shape\", x1.size())\n",
    "        x2 = F.relu(self.conv2(x))\n",
    "#         print(\"x2 shape \", x2.size())\n",
    "\n",
    "        x1 = self.maxpool(x1)\n",
    "#         print(\"x1 shape\", x1.size())        \n",
    "        x2 = self.maxpool(x2)\n",
    "#         print(\"x2 shape\", x2.size())        \n",
    "        x = torch.cat((x1,x2), 1)\n",
    "        x = self.drop2(x)\n",
    "        \n",
    "        x = x.view((self.max_length//2),-1, 400).float()#### change dimensionality\n",
    "        \n",
    "        hidden = Variable(torch.cuda.FloatTensor(1, batch_size, 100).uniform_()) \n",
    "        \n",
    "#        print(\"before rnn \", x.size(), \"hidden size \",hidden.size())\n",
    "\n",
    "        output, _ = self.rnn(x,hidden)\n",
    "#         print(\"AFTER rnn \",output.size())\n",
    "        \n",
    "        x = output.contiguous().view(-1, (self.max_length//2)*100)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.drop3(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "#     def create_emb_layer(self, weights_matrix, non_trainable=True):\n",
    "#         num_embeddings, embedding_dim = weights_matrix.shape\n",
    "#         emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
    "#         emb_layer.load_state_dict({'weight': weights_matrix})\n",
    "#         if non_trainable:\n",
    "#             emb_layer.weight.requires_grad = False\n",
    "\n",
    "#         return emb_layer, num_embeddings, embedding_dim\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch.callbacks import EpochScoring, PrintLog, ProgressBar, LRScheduler, EarlyStopping\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ds_accuracy(net, ds, y=None):\n",
    "    # assume ds yields (X, y), e.g. torchvision.datasets.MNIST\n",
    "    y_true = [y for _, y in ds]\n",
    "    y_pred = net.predict(ds)\n",
    "    return sklearn.metrics.accuracy_score(y_true, y_pred)\n",
    "    \n",
    "\n",
    "callbacks = [\n",
    "    ('es1',EpochScoring('accuracy')),\n",
    "    ('lrs',LRScheduler()),\n",
    "    ('est',EarlyStopping()) \n",
    "]\n",
    "\n",
    "batch_size = 256\n",
    "device = torch.device(\"cuda:0\")\n",
    "model = ReviewModel(data_loader.X_train.shape[1], batch_size).to(device)\n",
    "net = NeuralNetClassifier(model,## change dimensionality\n",
    "                          iterator_train__drop_last = True,\n",
    "                          iterator_valid__drop_last = True, \n",
    "                          max_epochs=30, \n",
    "                          lr=0.01, \n",
    "                          criterion = nn.CrossEntropyLoss, \n",
    "                          optimizer=optim.SGD,\n",
    "                          batch_size = batch_size,\n",
    "                          callbacks = callbacks,\n",
    "                          device = torch.device('cuda:0')\n",
    "                          )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cublas runtime error : the GPU program failed to execute at /opt/conda/conda-bld/pytorch_1549635019666/work/aten/src/THC/THCBlas.cu:258",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-6999efa383ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# labels = labels[:-rest]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/thesis/lib/python3.7/site-packages/skorch/classifier.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;31m# this is actually a pylint bug:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;31m# https://github.com/PyCQA/pylint/issues/1085\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNeuralNetClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/thesis/lib/python3.7/site-packages/skorch/net.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    840\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 842\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    843\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/thesis/lib/python3.7/site-packages/skorch/net.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, y, classes, **fit_params)\u001b[0m\n\u001b[1;32m    799\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'on_train_begin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 801\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    802\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/thesis/lib/python3.7/site-packages/skorch/net.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(self, X, y, epochs, **fit_params)\u001b[0m\n\u001b[1;32m    737\u001b[0m                 \u001b[0myi_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myi\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0my_train_is_ph\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'on_batch_begin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mXi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0myi_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m                 \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    740\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_batch_size'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_len\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/thesis/lib/python3.7/site-packages/skorch/net.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, Xi, yi, **fit_params)\u001b[0m\n\u001b[1;32m    666\u001b[0m             \u001b[0mstep_accumulator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstep_accumulator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/thesis/lib/python3.7/site-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mclosure\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/thesis/lib/python3.7/site-packages/skorch/net.py\u001b[0m in \u001b[0;36mstep_fn\u001b[0;34m()\u001b[0m\n\u001b[1;32m    663\u001b[0m         \u001b[0mstep_accumulator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_train_step_accumulator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 665\u001b[0;31m             \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    666\u001b[0m             \u001b[0mstep_accumulator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/thesis/lib/python3.7/site-packages/skorch/net.py\u001b[0m in \u001b[0;36mtrain_step_single\u001b[0;34m(self, Xi, yi, **fit_params)\u001b[0m\n\u001b[1;32m    607\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mXi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 609\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m         self.notify(\n",
      "\u001b[0;32m~/.conda/envs/thesis/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/thesis/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cublas runtime error : the GPU program failed to execute at /opt/conda/conda-bld/pytorch_1549635019666/work/aten/src/THC/THCBlas.cu:258"
     ]
    }
   ],
   "source": [
    "inputs = torch.from_numpy(data_loader.X_train).long().to(device)\n",
    "labels = torch.tensor(data_loader.y_train.values).long().to(device)\n",
    "# rest = inputs.size()[0] % batch_size\n",
    "# print(rest)\n",
    "# print(inputs.size())\n",
    "# inputs = inputs[:-rest]\n",
    "# print(inputs.size()[0]/batch_size)\n",
    "# print(inputs.shape)\n",
    "# labels = labels[:-rest]\n",
    "\n",
    "net.fit(inputs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.get_params()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = data_loader.indicesMatrix()\n",
    "import sklearn\n",
    "sorted(sklearn.metrics.SCORERS.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_losses(tr_loss, val_loss):\n",
    "    plt.plot(tr_loss, label=\"training\")\n",
    "    plt.plot(val_loss, label=\"validation\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo plot losses\n",
    "#todo plot accuracy\n",
    "#todo confusion matrix\n",
    "history = net.history\n",
    "train_losses = history[:, 'train_loss']\n",
    "valid_losses = history[:, 'valid_loss']\n",
    "\n",
    "accuracy = history[:, 'accuracy']\n",
    "plot_losses(train_losses, valid_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.tensor(weights_matrix).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Variable(w.double())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
